

🏷 #Prompt优化 #人类反馈 #LLM

## **一、研究背景**

- **问题**：大语言模型（LLMs）的效果高度依赖提示词，但传统优化方法通常需要：
    
    - 明确的质量评价指标（如分类准确率）
        
    - 大量人类反馈或对照答案

- **难点**：
    
    - 对于开放式生成任务（如笑话创作、作文批改），没有标准答案，评价困难。
        
    - 用 GPT-4 之类模型充当评价者，可能与真实人类偏好差异较大。
        
    - 人类专家反馈昂贵且难以规模化。

---

## **二、核心贡献**

1. **提出 PLHF 框架**（Prompt Learning with Human Feedback）
    
    - 借鉴 RLHF 思路，但仅需**一次人类反馈**（few-shot）即可完成优化。
        
    - 设计 **双模块架构**：
        
        - **Responder R**：执行主任务（如生成回答、作文评分）。
            
        - **Evaluator E**：模仿人类评分偏好，成为自动化的“评价器”。

2. **解决了无明确定义指标的任务优化问题**，降低人类反馈依赖。
    
3. **实验证明**：在公共数据集与工业级任务中，PLHF优于常规方法（如Exact Match、Embedding相似度、GPT-4o作为评价器）。

---

## **三、方法设计**

```mermaid
flowchart LR
    HF[人类反馈 一次] -.-> A
    A[输入样本 input output 分数] --> B[评价器 E]
    B --> B2[优化后评价器 PE']
    B2 --> C[回答器 R 输出]
    C --> C2[评价器 E 打分]
    C2 --> D[优化提示词 PR' 和 PE']
```

### **1. Evaluator 模块 (E)**

- 任务：学习模仿人类打分（回归/分类任务）。
    
- 数据：由人类对小样本打分生成 **(输入, 输出, 分数)** 三元组。
    
- 优化：用简单指标（准确率/MAE）优化提示词，使其能接近人类评分。

### **2. Responder 模块 (R)**

- 任务：执行生成任务。
    
- 评估：由 Evaluator E 打分 → 用此分数优化提示词 P’R。
    
- 好处：只需一次人类反馈，后续由 E 代替人类持续优化。

### **3. 整体流程**

1. 用少量样本请人类打分。
    
2. 训练 Evaluator E 模拟人类偏好。
    
3. 用优化好的 E 作为“代理评价器”，指导 Responder R 的提示词优化。
    
4. 迭代更新，得到更优提示词。

---

## **四、实验设计**

### **数据集**

1. **Schema Guided Dialogue (SGD)**：任务型对话数据（含用户满意度打分）。
    
2. **AES-ASAP & AES-2.0**：自动作文评分（人类专家打分）。
    
3. **SQL-QA（工业数据）**：真实客服问答场景，生成 SQL 查询。

### **对比方法**

- GPT-3.5 基线
    
- Exact Matching / Embedding 相似度
    
- GPT-4o 作为评估器
    
- DSPy / TextGrad 框架
    
- **PLHF**

---

## **五、实验结果**

### **1. Evaluator 子任务（预测人类打分）**

- **传统方法（MLP/SVM + Embedding）表现较差**
    
- **LLM + 提示优化显著更好**
    
- DSPy/TextGrad 优化后，PLHF 最佳

### **2. Responder 主任务（输出质量）**

- PLHF 在所有数据集上**提升最显著**，甚至超过使用 GPT-4o 作为评价器的方法。

|**方法**|**SGD**|**AES-ASAP**|**AES-2.0**|**SQL-QA**|
|---|---|---|---|---|
|GPT-3.5 基线|4.25|26.50|5.35|0.74|
|GPT-4o 评价器|+1–5%|+3–4%|+1%|+2–5%|
|Exact/Embedding|不稳定|有时负增益|-|劣于基线|
|**PLHF**|**+6.6%**|**+8.5%**|**+4.3%**|**+18.9%**|

---

## **六、结论**

- **PLHF 的优势**：
    
    - 仅需一次人类反馈，后续由 Evaluator 模块接管，效率高。
        
    - 在多任务测试中，生成质量显著优于现有自动优化方法。
        
    - 特别适合 **无标准答案 / 多样化输出** 的任务（作文评分、笑话生成、开放问答）。

- **未来方向**：
    
    - 迁移到多模态任务（如图文生成）。
        
    - 结合更多少样本优化技术（例如自我反馈、演化式提示词搜索）。

---

## **七、批判性思考**

- **优势**：
    
    - 在减少人力成本的同时保持了 RLHF 的核心思想。
        
    - 框架通用，可嵌入到现有 DSPy / TextGrad 等系统。

- **潜在问题**：
    
    - Evaluator 仍基于 LLM，可能继承其偏见与盲点。
        
    - 一次性人类反馈是否足够泛化，取决于样本多样性。
        
    - 对于极高安全性需求场景（如医疗、法律），可能仍需更多人工复核。

- **反思**：
    
    - PLHF 并非完全替代 RLHF，而是一种**轻量化、性价比更高**的替代方案，适合资源有限的企业或研究团队。
