# æ·±åº¦å­¦ä¹ æ­£åˆ™åŒ–æŠ€æœ¯å…¨é¢æŒ‡å—

> **æ ‡ç­¾**: æ­£åˆ™åŒ– | è¿‡æ‹Ÿåˆ | æ¨¡å‹æ³›åŒ– | è®­ç»ƒæŠ€å·§  
> **é€‚ç”¨åœºæ™¯**: æ¨¡å‹è®­ç»ƒã€æ€§èƒ½ä¼˜åŒ–ã€æ³›åŒ–èƒ½åŠ›æå‡  
> **éš¾åº¦çº§åˆ«**: â­â­â­â­

## ğŸ“‹ æ¦‚è¿°

æ­£åˆ™åŒ–ï¼ˆRegularizationï¼‰æ˜¯æ·±åº¦å­¦ä¹ ä¸­é˜²æ­¢è¿‡æ‹Ÿåˆã€æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„æ ¸å¿ƒæŠ€æœ¯ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥çº¦æŸæˆ–å™ªå£°ï¼Œæ­£åˆ™åŒ–æŠ€æœ¯å¸®åŠ©æ¨¡å‹åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸Šå­¦åˆ°æ›´é€šç”¨çš„ç‰¹å¾è¡¨ç¤ºã€‚æœ¬æ–‡å°†å…¨é¢ä»‹ç»å„ç±»æ­£åˆ™åŒ–æŠ€æœ¯çš„åŸç†ã€å®ç°å’Œåº”ç”¨ç­–ç•¥ã€‚

---

## ğŸ¯ ä¸€ã€æ­£åˆ™åŒ–æŠ€æœ¯åˆ†ç±»ä½“ç³»

### 1.1 æŒ‰ä½œç”¨æœºåˆ¶åˆ†ç±»

```
æ­£åˆ™åŒ–æŠ€æœ¯åˆ†ç±»
â”œâ”€â”€ å‚æ•°æ­£åˆ™åŒ–
â”‚   â”œâ”€â”€ L1æ­£åˆ™åŒ– (Lasso)
â”‚   â”œâ”€â”€ L2æ­£åˆ™åŒ– (Ridge)
â”‚   â”œâ”€â”€ å¼¹æ€§ç½‘ç»œ (Elastic Net)
â”‚   â””â”€â”€ æƒé‡è¡°å‡ (Weight Decay)
â”œâ”€â”€ ç»“æ„æ­£åˆ™åŒ–
â”‚   â”œâ”€â”€ Dropout
â”‚   â”œâ”€â”€ DropConnect
â”‚   â”œâ”€â”€ DropPath
â”‚   â””â”€â”€ Stochastic Depth
â”œâ”€â”€ æ•°æ®æ­£åˆ™åŒ–
â”‚   â”œâ”€â”€ Data Augmentation
â”‚   â”œâ”€â”€ Mixup
â”‚   â”œâ”€â”€ CutMix
â”‚   â””â”€â”€ Label Smoothing
â”œâ”€â”€ å½’ä¸€åŒ–æŠ€æœ¯
â”‚   â”œâ”€â”€ Batch Normalization
â”‚   â”œâ”€â”€ Layer Normalization
â”‚   â”œâ”€â”€ Group Normalization
â”‚   â””â”€â”€ Instance Normalization
â”œâ”€â”€ æ—©åœä¸è°ƒåº¦
â”‚   â”œâ”€â”€ Early Stopping
â”‚   â”œâ”€â”€ Learning Rate Scheduling
â”‚   â””â”€â”€ Gradient Clipping
â””â”€â”€ é«˜çº§æŠ€æœ¯
    â”œâ”€â”€ Spectral Normalization
    â”œâ”€â”€ Noise Injection
    â””â”€â”€ Adversarial Training
```

---

## ğŸ”§ äºŒã€å‚æ•°æ­£åˆ™åŒ–æŠ€æœ¯

### 2.1 L1æ­£åˆ™åŒ– (Lasso Regularization)

#### æ•°å­¦åŸç†
```
L1_regularized_loss = original_loss + Î» Ã— Î£|wi|

å…¶ä¸­ï¼š
- Î»ï¼šæ­£åˆ™åŒ–å¼ºåº¦ç³»æ•°
- wiï¼šæ¨¡å‹å‚æ•°
```

#### ç‰¹æ€§åˆ†æ
- **ç¨€ç–æ€§**: å€¾å‘äºäº§ç”Ÿç¨€ç–è§£ï¼Œå°†ä¸é‡è¦çš„æƒé‡ç½®ä¸º0
- **ç‰¹å¾é€‰æ‹©**: è‡ªåŠ¨è¿›è¡Œç‰¹å¾é€‰æ‹©
- **ä¸å¯å¾®**: åœ¨0ç‚¹ä¸å¯å¾®åˆ†

#### ä»£ç å®ç°
```python
import torch
import torch.nn as nn

class L1Regularization:
    def __init__(self, lambda_reg=0.01):
        self.lambda_reg = lambda_reg
    
    def __call__(self, model):
        """è®¡ç®—L1æ­£åˆ™åŒ–é¡¹"""
        l1_penalty = torch.tensor(0.0, requires_grad=True)
        
        for param in model.parameters():
            l1_penalty = l1_penalty + torch.norm(param, p=1)
        
        return self.lambda_reg * l1_penalty

# ä½¿ç”¨ç¤ºä¾‹
class RegularizedModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear1 = nn.Linear(784, 256)
        self.linear2 = nn.Linear(256, 128)
        self.linear3 = nn.Linear(128, 10)
        self.relu = nn.ReLU()
        self.l1_reg = L1Regularization(lambda_reg=0.01)
    
    def forward(self, x):
        x = self.relu(self.linear1(x))
        x = self.relu(self.linear2(x))
        x = self.linear3(x)
        return x
    
    def get_l1_loss(self):
        return self.l1_reg(self)

# è®­ç»ƒå¾ªç¯ä¸­çš„ä½¿ç”¨
def train_with_l1(model, dataloader, optimizer, criterion):
    model.train()
    total_loss = 0
    
    for data, target in dataloader:
        optimizer.zero_grad()
        
        output = model(data)
        classification_loss = criterion(output, target)
        l1_loss = model.get_l1_loss()
        
        total_loss = classification_loss + l1_loss
        total_loss.backward()
        optimizer.step()
    
    return total_loss
```

#### é€‚ç”¨åœºæ™¯
- ç‰¹å¾æ•°é‡è¿œå¤§äºæ ·æœ¬æ•°é‡
- éœ€è¦è‡ªåŠ¨ç‰¹å¾é€‰æ‹©
- è§£é‡Šæ€§è¦æ±‚è¾ƒé«˜çš„æ¨¡å‹

### 2.2 L2æ­£åˆ™åŒ– (Ridge Regularization)

#### æ•°å­¦åŸç†
```
L2_regularized_loss = original_loss + Î» Ã— Î£wiÂ²
```

#### ç‰¹æ€§åˆ†æ
- **å¹³æ»‘æ€§**: å€¾å‘äºå°†æƒé‡å‡åŒ€åˆ†å¸ƒ
- **ç¨³å®šæ€§**: å¯¹å™ªå£°å…·æœ‰è¾ƒå¥½çš„é²æ£’æ€§
- **å¯å¾®æ€§**: å¤„å¤„å¯å¾®ï¼Œä¾¿äºä¼˜åŒ–

#### ä»£ç å®ç°
```python
class L2Regularization:
    def __init__(self, lambda_reg=0.01):
        self.lambda_reg = lambda_reg
    
    def __call__(self, model):
        """è®¡ç®—L2æ­£åˆ™åŒ–é¡¹"""
        l2_penalty = torch.tensor(0.0, requires_grad=True)
        
        for param in model.parameters():
            l2_penalty = l2_penalty + torch.norm(param, p=2)**2
        
        return self.lambda_reg * l2_penalty

# æƒé‡è¡°å‡çš„ç­‰ä»·å®ç°
# åœ¨ä¼˜åŒ–å™¨ä¸­ç›´æ¥è®¾ç½®weight_decayå‚æ•°
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)

# è‡ªå®šä¹‰æƒé‡è¡°å‡
class WeightDecayOptimizer:
    def __init__(self, optimizer, weight_decay=0.01):
        self.optimizer = optimizer
        self.weight_decay = weight_decay
    
    def step(self):
        # åº”ç”¨æƒé‡è¡°å‡
        for group in self.optimizer.param_groups:
            for param in group['params']:
                if param.grad is not None:
                    param.grad.data.add_(param.data, alpha=self.weight_decay)
        
        # æ‰§è¡Œä¼˜åŒ–æ­¥éª¤
        self.optimizer.step()
    
    def zero_grad(self):
        self.optimizer.zero_grad()
```

#### L1 vs L2 æ¯”è¾ƒåˆ†æ
```python
def compare_l1_l2_regularization(X, y, lambda_values):
    """æ¯”è¾ƒL1å’ŒL2æ­£åˆ™åŒ–çš„æ•ˆæœ"""
    import matplotlib.pyplot as plt
    
    results = {'L1': [], 'L2': [], 'lambda': lambda_values}
    
    for lam in lambda_values:
        # L1æ­£åˆ™åŒ–æ¨¡å‹
        model_l1 = create_model_with_l1(lam)
        train_model(model_l1, X, y)
        sparsity_l1 = calculate_sparsity(model_l1)
        results['L1'].append(sparsity_l1)
        
        # L2æ­£åˆ™åŒ–æ¨¡å‹
        model_l2 = create_model_with_l2(lam)
        train_model(model_l2, X, y)
        sparsity_l2 = calculate_sparsity(model_l2)
        results['L2'].append(sparsity_l2)
    
    # å¯è§†åŒ–æ¯”è¾ƒ
    plt.figure(figsize=(10, 6))
    plt.plot(lambda_values, results['L1'], 'o-', label='L1 Regularization')
    plt.plot(lambda_values, results['L2'], 's-', label='L2 Regularization')
    plt.xlabel('Lambda (Regularization Strength)')
    plt.ylabel('Model Sparsity')
    plt.title('L1 vs L2 Regularization Effect on Model Sparsity')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    
    return results

def calculate_sparsity(model):
    """è®¡ç®—æ¨¡å‹çš„ç¨€ç–æ€§"""
    total_params = 0
    zero_params = 0
    
    for param in model.parameters():
        total_params += param.numel()
        zero_params += (torch.abs(param) < 1e-6).sum().item()
    
    return zero_params / total_params
```

### 2.3 å¼¹æ€§ç½‘ç»œ (Elastic Net)

#### æ•°å­¦åŸç†
```
Elastic_loss = original_loss + Î»1Ã—Î£|wi| + Î»2Ã—Î£wiÂ²
```

#### ä»£ç å®ç°
```python
class ElasticNetRegularization:
    def __init__(self, lambda_l1=0.01, lambda_l2=0.01):
        self.lambda_l1 = lambda_l1
        self.lambda_l2 = lambda_l2
    
    def __call__(self, model):
        """è®¡ç®—å¼¹æ€§ç½‘ç»œæ­£åˆ™åŒ–é¡¹"""
        l1_penalty = torch.tensor(0.0, requires_grad=True)
        l2_penalty = torch.tensor(0.0, requires_grad=True)
        
        for param in model.parameters():
            l1_penalty = l1_penalty + torch.norm(param, p=1)
            l2_penalty = l2_penalty + torch.norm(param, p=2)**2
        
        return self.lambda_l1 * l1_penalty + self.lambda_l2 * l2_penalty

# è‡ªé€‚åº”æ­£åˆ™åŒ–å¼ºåº¦
class AdaptiveElasticNet:
    def __init__(self, initial_l1=0.01, initial_l2=0.01):
        self.lambda_l1 = initial_l1
        self.lambda_l2 = initial_l2
        self.loss_history = []
    
    def update_lambdas(self, current_loss, target_sparsity=0.3):
        """æ ¹æ®è®­ç»ƒè¿›åº¦è‡ªé€‚åº”è°ƒæ•´æ­£åˆ™åŒ–å¼ºåº¦"""
        self.loss_history.append(current_loss)
        
        if len(self.loss_history) > 10:
            # å¦‚æœæŸå¤±ä¸‹é™ç¼“æ…¢ï¼Œå¢åŠ æ­£åˆ™åŒ–
            recent_improvement = self.loss_history[-10] - current_loss
            if recent_improvement < 0.01:
                self.lambda_l1 *= 1.1
                self.lambda_l2 *= 1.05
            # å¦‚æœæŸå¤±ä¸‹é™è¿‡å¿«ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆï¼Œå¢åŠ æ­£åˆ™åŒ–
            elif recent_improvement > 0.1:
                self.lambda_l1 *= 1.2
                self.lambda_l2 *= 1.1
    
    def __call__(self, model):
        l1_penalty = sum(torch.norm(p, p=1) for p in model.parameters())
        l2_penalty = sum(torch.norm(p, p=2)**2 for p in model.parameters())
        
        return self.lambda_l1 * l1_penalty + self.lambda_l2 * l2_penalty
```

---

## ğŸ—ï¸ ä¸‰ã€ç»“æ„æ­£åˆ™åŒ–æŠ€æœ¯

### 3.1 Dropout

#### åŸºæœ¬åŸç†
åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºå°†ä¸€å®šæ¯”ä¾‹çš„ç¥ç»å…ƒè¾“å‡ºç½®ä¸º0ï¼Œå¼ºåˆ¶ç½‘ç»œä¸ä¾èµ–ç‰¹å®šçš„ç¥ç»å…ƒã€‚

#### æ•°å­¦è¡¨è¾¾
```
y = dropout(x, p) = {
    x / (1-p)  with probability (1-p)
    0          with probability p
}
```

#### ä»£ç å®ç°
```python
class Dropout(nn.Module):
    def __init__(self, p=0.5):
        super().__init__()
        self.p = p
    
    def forward(self, x):
        if self.training:
            # ç”Ÿæˆéšæœºæ©ç 
            mask = torch.bernoulli(torch.full_like(x, 1-self.p))
            # åº”ç”¨æ©ç å¹¶ç¼©æ”¾
            return x * mask / (1-self.p)
        else:
            return x

# è‡ªé€‚åº”Dropout
class AdaptiveDropout(nn.Module):
    def __init__(self, initial_p=0.5, min_p=0.1, max_p=0.8):
        super().__init__()
        self.p = initial_p
        self.min_p = min_p
        self.max_p = max_p
        self.epoch = 0
    
    def update_dropout_rate(self, val_loss, train_loss):
        """æ ¹æ®è¿‡æ‹Ÿåˆç¨‹åº¦è°ƒæ•´dropoutç‡"""
        overfitting_ratio = val_loss / train_loss
        
        if overfitting_ratio > 1.3:  # æ˜æ˜¾è¿‡æ‹Ÿåˆ
            self.p = min(self.p * 1.1, self.max_p)
        elif overfitting_ratio < 1.1:  # æ¬ æ‹Ÿåˆ
            self.p = max(self.p * 0.9, self.min_p)
        
        print(f"Dropout rate updated to: {self.p:.3f}")
    
    def forward(self, x):
        if self.training:
            mask = torch.bernoulli(torch.full_like(x, 1-self.p))
            return x * mask / (1-self.p)
        else:
            return x

# å˜åˆ†Dropout (Variational Dropout)
class VariationalDropout(nn.Module):
    def __init__(self, input_dim, output_dim, log_sigma2_init=-10):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        
        # æƒé‡çš„å‡å€¼å’Œæ–¹å·®å‚æ•°
        self.weight_mu = nn.Parameter(torch.randn(output_dim, input_dim))
        self.weight_log_sigma2 = nn.Parameter(torch.full((output_dim, input_dim), log_sigma2_init))
        
        # åç½®
        self.bias = nn.Parameter(torch.zeros(output_dim))
    
    def forward(self, x):
        if self.training:
            # é‡‡æ ·æƒé‡
            weight_sigma = torch.exp(0.5 * self.weight_log_sigma2)
            epsilon = torch.randn_like(self.weight_mu)
            weight = self.weight_mu + weight_sigma * epsilon
        else:
            weight = self.weight_mu
        
        return F.linear(x, weight, self.bias)
    
    def kl_divergence(self):
        """è®¡ç®—KLæ•£åº¦æ­£åˆ™åŒ–é¡¹"""
        # è¿‘ä¼¼è®¡ç®—KLæ•£åº¦
        log_alpha = self.weight_log_sigma2 - 2 * torch.log(torch.abs(self.weight_mu) + 1e-8)
        kl = -0.5 * torch.sum(1 + log_alpha - torch.pow(self.weight_mu, 2) - torch.exp(log_alpha))
        return kl
```

#### ä¸åŒç±»å‹çš„Dropoutæ¯”è¾ƒ
```python
class DropoutComparison:
    @staticmethod
    def standard_dropout(x, p=0.5):
        """æ ‡å‡†Dropout"""
        if not torch.is_tensor(x):
            x = torch.tensor(x)
        mask = torch.bernoulli(torch.full_like(x, 1-p))
        return x * mask / (1-p)
    
    @staticmethod
    def gaussian_dropout(x, p=0.5):
        """é«˜æ–¯Dropout"""
        if not torch.is_tensor(x):
            x = torch.tensor(x)
        std = (p / (1-p))**0.5
        noise = torch.normal(1.0, std, size=x.shape)
        return x * noise
    
    @staticmethod
    def alpha_dropout(x, p=0.5):
        """Alpha Dropout (ç”¨äºSELUæ¿€æ´»å‡½æ•°)"""
        if not torch.is_tensor(x):
            x = torch.tensor(x)
            
        alpha = 1.7580993408473766
        scale = 1.0507009873554805
        
        a = ((1-p) * (1 + p * alpha**2))**-0.5
        b = -a * alpha * p
        
        mask = torch.bernoulli(torch.full_like(x, 1-p))
        x = mask * x + (1-mask) * alpha
        
        return a * x + b
    
    @staticmethod
    def compare_dropout_types(input_data, dropout_rate=0.5):
        """æ¯”è¾ƒä¸åŒDropoutç±»å‹çš„æ•ˆæœ"""
        results = {}
        
        results['standard'] = DropoutComparison.standard_dropout(input_data, dropout_rate)
        results['gaussian'] = DropoutComparison.gaussian_dropout(input_data, dropout_rate)
        results['alpha'] = DropoutComparison.alpha_dropout(input_data, dropout_rate)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        for name, output in results.items():
            mean_val = torch.mean(output).item()
            std_val = torch.std(output).item()
            print(f"{name.capitalize()} Dropout - Mean: {mean_val:.4f}, Std: {std_val:.4f}")
        
        return results
```

### 3.2 DropConnect

#### åŸç†ä¸å®ç°
DropConnectéšæœºä¸¢å¼ƒè¿æ¥è€Œä¸æ˜¯ç¥ç»å…ƒï¼Œå¯¹æƒé‡çŸ©é˜µè¿›è¡Œéšæœºæ©ç ã€‚

```python
class DropConnect(nn.Module):
    def __init__(self, input_dim, output_dim, drop_prob=0.5):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.drop_prob = drop_prob
        
        self.weight = nn.Parameter(torch.randn(output_dim, input_dim))
        self.bias = nn.Parameter(torch.zeros(output_dim))
    
    def forward(self, x):
        if self.training:
            # å¯¹æƒé‡åº”ç”¨DropConnect
            mask = torch.bernoulli(torch.full_like(self.weight, 1-self.drop_prob))
            masked_weight = self.weight * mask / (1-self.drop_prob)
        else:
            masked_weight = self.weight
        
        return F.linear(x, masked_weight, self.bias)

# DropConnectä¸Dropoutçš„æ•ˆæœæ¯”è¾ƒ
class DropoutVsDropConnect:
    def __init__(self, input_dim, hidden_dim, output_dim, drop_rate=0.5):
        self.dropout_model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(drop_rate),
            nn.Linear(hidden_dim, output_dim)
        )
        
        self.dropconnect_model = nn.Sequential(
            DropConnect(input_dim, hidden_dim, drop_rate),
            nn.ReLU(),
            DropConnect(hidden_dim, output_dim, drop_rate)
        )
    
    def compare_regularization_effect(self, train_loader, val_loader, epochs=10):
        """æ¯”è¾ƒä¸¤ç§æ­£åˆ™åŒ–æ–¹æ³•çš„æ•ˆæœ"""
        
        # è®­ç»ƒä¸¤ä¸ªæ¨¡å‹
        dropout_results = self.train_model(self.dropout_model, train_loader, val_loader, epochs)
        dropconnect_results = self.train_model(self.dropconnect_model, train_loader, val_loader, epochs)
        
        return {
            'dropout': dropout_results,
            'dropconnect': dropconnect_results
        }
```

### 3.3 DropPath (Stochastic Depth)

#### å®ç°
```python
class DropPath(nn.Module):
    """éšæœºæ·±åº¦ï¼šéšæœºè·³è¿‡æŸäº›å±‚"""
    def __init__(self, drop_prob=0.1):
        super().__init__()
        self.drop_prob = drop_prob
    
    def forward(self, x):
        if not self.training or self.drop_prob == 0.0:
            return x
        
        keep_prob = 1 - self.drop_prob
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # ä¿æŒbatchç»´åº¦
        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
        random_tensor.floor_()  # äºŒå€¼åŒ–
        
        output = x.div(keep_prob) * random_tensor
        return output

# åœ¨ResNetä¸­ä½¿ç”¨DropPath
class ResidualBlockWithDropPath(nn.Module):
    def __init__(self, in_channels, out_channels, drop_path_rate=0.1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.drop_path = DropPath(drop_path_rate)
        
        # å¿«æ·è¿æ¥
        if in_channels != out_channels:
            self.shortcut = nn.Conv2d(in_channels, out_channels, 1)
        else:
            self.shortcut = nn.Identity()
    
    def forward(self, x):
        identity = self.shortcut(x)
        
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        
        # åº”ç”¨DropPath
        out = self.drop_path(out)
        
        out += identity
        out = self.relu(out)
        
        return out
```

---

## ğŸ“Š å››ã€å½’ä¸€åŒ–æŠ€æœ¯

### 4.1 Batch Normalization

#### æ•°å­¦åŸç†
```
BN(x) = Î³ Ã— (x - Î¼_batch) / Ïƒ_batch + Î²

å…¶ä¸­ï¼š
- Î¼_batchï¼šæ‰¹æ¬¡å‡å€¼
- Ïƒ_batchï¼šæ‰¹æ¬¡æ ‡å‡†å·®
- Î³, Î²ï¼šå¯å­¦ä¹ å‚æ•°
```

#### ä»£ç å®ç°
```python
class BatchNorm1d(nn.Module):
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        super().__init__()
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        
        # å¯å­¦ä¹ å‚æ•°
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))
        
        # è¿è¡Œæ—¶ç»Ÿè®¡é‡
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
    
    def forward(self, x):
        if self.training:
            # è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨å½“å‰æ‰¹æ¬¡ç»Ÿè®¡é‡
            batch_mean = x.mean(dim=0)
            batch_var = x.var(dim=0, unbiased=False)
            
            # æ›´æ–°è¿è¡Œæ—¶ç»Ÿè®¡é‡
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var
            
            # å½’ä¸€åŒ–
            x_norm = (x - batch_mean) / torch.sqrt(batch_var + self.eps)
        else:
            # æ¨ç†æ¨¡å¼ï¼šä½¿ç”¨è¿è¡Œæ—¶ç»Ÿè®¡é‡
            x_norm = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)
        
        # ç¼©æ”¾å’Œå¹³ç§»
        return self.gamma * x_norm + self.beta

# åˆ†æBatchNormçš„æ­£åˆ™åŒ–æ•ˆæœ
class BatchNormAnalysis:
    @staticmethod
    def analyze_gradient_flow(model_with_bn, model_without_bn, input_data):
        """åˆ†æBatchNormå¯¹æ¢¯åº¦æµçš„å½±å“"""
        
        def get_gradient_norms(model, data):
            model.train()
            output = model(data)
            loss = output.sum()  # ç®€å•çš„æŸå¤±å‡½æ•°
            loss.backward()
            
            grad_norms = []
            for name, param in model.named_parameters():
                if param.grad is not None:
                    grad_norms.append(param.grad.norm().item())
            
            model.zero_grad()
            return grad_norms
        
        grad_norms_with_bn = get_gradient_norms(model_with_bn, input_data)
        grad_norms_without_bn = get_gradient_norms(model_without_bn, input_data)
        
        return {
            'with_bn': grad_norms_with_bn,
            'without_bn': grad_norms_without_bn,
            'ratio': [a/b for a, b in zip(grad_norms_with_bn, grad_norms_without_bn)]
        }
    
    @staticmethod
    def visualize_activation_distribution(model, input_data, layer_names):
        """å¯è§†åŒ–æ¿€æ´»åˆ†å¸ƒ"""
        import matplotlib.pyplot as plt
        
        activations = {}
        
        def hook_fn(name):
            def hook(module, input, output):
                activations[name] = output.detach()
            return hook
        
        # æ³¨å†Œé’©å­
        hooks = []
        for name, module in model.named_modules():
            if name in layer_names:
                hook = module.register_forward_hook(hook_fn(name))
                hooks.append(hook)
        
        # å‰å‘ä¼ æ’­
        model.eval()
        with torch.no_grad():
            _ = model(input_data)
        
        # å¯è§†åŒ–
        fig, axes = plt.subplots(2, len(layer_names)//2, figsize=(15, 8))
        axes = axes.flatten()
        
        for i, (name, activation) in enumerate(activations.items()):
            axes[i].hist(activation.flatten().numpy(), bins=50, alpha=0.7)
            axes[i].set_title(f'Layer: {name}')
            axes[i].set_xlabel('Activation Value')
            axes[i].set_ylabel('Frequency')
        
        plt.tight_layout()
        plt.show()
        
        # æ¸…ç†é’©å­
        for hook in hooks:
            hook.remove()
```

### 4.2 Layer Normalization

#### ä»£ç å®ç°
```python
class LayerNorm(nn.Module):
    def __init__(self, normalized_shape, eps=1e-5):
        super().__init__()
        self.normalized_shape = normalized_shape
        self.eps = eps
        
        self.gamma = nn.Parameter(torch.ones(normalized_shape))
        self.beta = nn.Parameter(torch.zeros(normalized_shape))
    
    def forward(self, x):
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬åœ¨ç‰¹å¾ç»´åº¦ä¸Šçš„å‡å€¼å’Œæ–¹å·®
        dims = [-i for i in range(1, len(self.normalized_shape) + 1)]
        mean = x.mean(dim=dims, keepdim=True)
        var = x.var(dim=dims, unbiased=False, keepdim=True)
        
        # å½’ä¸€åŒ–
        x_norm = (x - mean) / torch.sqrt(var + self.eps)
        
        return self.gamma * x_norm + self.beta

# æ¯”è¾ƒä¸åŒå½’ä¸€åŒ–æ–¹æ³•
class NormalizationComparison:
    def __init__(self, input_shape):
        self.batch_norm = nn.BatchNorm2d(input_shape[1])
        self.layer_norm = nn.LayerNorm(input_shape[1:])  # é™¤batchç»´åº¦å¤–çš„æ‰€æœ‰ç»´åº¦
        self.group_norm = nn.GroupNorm(8, input_shape[1])  # 8ä¸ªç»„
        self.instance_norm = nn.InstanceNorm2d(input_shape[1])
    
    def compare_normalization_effects(self, input_data):
        """æ¯”è¾ƒä¸åŒå½’ä¸€åŒ–æ–¹æ³•çš„æ•ˆæœ"""
        results = {}
        
        methods = {
            'BatchNorm': self.batch_norm,
            'LayerNorm': self.layer_norm,
            'GroupNorm': self.group_norm,
            'InstanceNorm': self.instance_norm
        }
        
        for name, norm_layer in methods.items():
            try:
                normalized = norm_layer(input_data)
                
                results[name] = {
                    'mean': normalized.mean().item(),
                    'std': normalized.std().item(),
                    'min': normalized.min().item(),
                    'max': normalized.max().item(),
                    'shape': normalized.shape
                }
            except Exception as e:
                results[name] = {'error': str(e)}
        
        return results
```

### 4.3 Group Normalization

#### ä»£ç å®ç°
```python
class GroupNorm(nn.Module):
    def __init__(self, num_groups, num_channels, eps=1e-5):
        super().__init__()
        assert num_channels % num_groups == 0
        self.num_groups = num_groups
        self.num_channels = num_channels
        self.eps = eps
        
        self.gamma = nn.Parameter(torch.ones(num_channels))
        self.beta = nn.Parameter(torch.zeros(num_channels))
    
    def forward(self, x):
        N, C, H, W = x.shape
        
        # é‡å¡‘ä¸º [N, num_groups, C//num_groups, H, W]
        x = x.view(N, self.num_groups, C // self.num_groups, H, W)
        
        # è®¡ç®—æ¯ä¸ªç»„çš„å‡å€¼å’Œæ–¹å·®
        mean = x.mean(dim=[2, 3, 4], keepdim=True)
        var = x.var(dim=[2, 3, 4], unbiased=False, keepdim=True)
        
        # å½’ä¸€åŒ–
        x = (x - mean) / torch.sqrt(var + self.eps)
        
        # æ¢å¤åŸå§‹å½¢çŠ¶
        x = x.view(N, C, H, W)
        
        # ç¼©æ”¾å’Œå¹³ç§»
        return x * self.gamma.view(1, C, 1, 1) + self.beta.view(1, C, 1, 1)
```

---

## ğŸ“ˆ äº”ã€æ•°æ®æ­£åˆ™åŒ–æŠ€æœ¯

### 5.1 æ•°æ®å¢å¼º (Data Augmentation)

#### å›¾åƒæ•°æ®å¢å¼º
```python
import torchvision.transforms as transforms

class ImageAugmentation:
    def __init__(self):
        self.basic_augmentation = transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=10),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        self.strong_augmentation = transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=30),
            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),
            transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),
            transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.8, 1.2)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    def get_augmentation(self, strength='basic'):
        if strength == 'basic':
            return self.basic_augmentation
        elif strength == 'strong':
            return self.strong_augmentation
        else:
            raise ValueError("Strength must be 'basic' or 'strong'")

# è‡ªåŠ¨å¢å¼ºç­–ç•¥
class AutoAugment:
    def __init__(self, policies=None):
        if policies is None:
            self.policies = self.get_default_policies()
        else:
            self.policies = policies
    
    def get_default_policies(self):
        """å®šä¹‰é»˜è®¤çš„å¢å¼ºç­–ç•¥"""
        return [
            [('Rotate', 0.7, 6), ('TranslateX', 0.3, 4)],
            [('ShearY', 0.8, 8), ('Color', 0.6, 4)],
            [('Brightness', 0.6, 8), ('Contrast', 0.4, 3)],
            # æ›´å¤šç­–ç•¥...
        ]
    
    def apply_policy(self, image, policy):
        """åº”ç”¨å•ä¸ªç­–ç•¥"""
        for operation, probability, magnitude in policy:
            if torch.rand(1) < probability:
                image = self.apply_operation(image, operation, magnitude)
        return image
    
    def apply_operation(self, image, operation, magnitude):
        """åº”ç”¨å…·ä½“çš„å˜æ¢æ“ä½œ"""
        # å®ç°å…·ä½“çš„å˜æ¢é€»è¾‘
        transform_map = {
            'Rotate': lambda img, mag: transforms.functional.rotate(img, angle=mag),
            'TranslateX': lambda img, mag: transforms.functional.affine(
                img, angle=0, translate=[mag, 0], scale=1, shear=0),
            # æ›´å¤šå˜æ¢...
        }
        
        if operation in transform_map:
            return transform_map[operation](image, magnitude)
        return image
    
    def __call__(self, image):
        policy = random.choice(self.policies)
        return self.apply_policy(image, policy)
```

### 5.2 Mixup

#### åŸç†ä¸å®ç°
```python
class Mixup:
    def __init__(self, alpha=1.0):
        self.alpha = alpha
    
    def __call__(self, batch_x, batch_y):
        """
        Mixupæ•°æ®å¢å¼º
        
        Args:
            batch_x: è¾“å…¥æ‰¹æ¬¡ [batch_size, ...]
            batch_y: æ ‡ç­¾æ‰¹æ¬¡ [batch_size, num_classes]
        """
        if self.alpha > 0:
            lam = torch.distributions.Beta(self.alpha, self.alpha).sample()
        else:
            lam = 1
        
        batch_size = batch_x.size(0)
        index = torch.randperm(batch_size)
        
        # æ··åˆè¾“å…¥
        mixed_x = lam * batch_x + (1 - lam) * batch_x[index]
        
        # æ··åˆæ ‡ç­¾
        y_a, y_b = batch_y, batch_y[index]
        
        return mixed_x, y_a, y_b, lam
    
    def mixup_criterion(self, criterion, pred, y_a, y_b, lam):
        """MixupæŸå¤±å‡½æ•°"""
        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

# ä½¿ç”¨ç¤ºä¾‹
def train_with_mixup(model, train_loader, optimizer, criterion, mixup):
    model.train()
    total_loss = 0
    
    for batch_x, batch_y in train_loader:
        # åº”ç”¨Mixup
        mixed_x, y_a, y_b, lam = mixup(batch_x, batch_y)
        
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        output = model(mixed_x)
        
        # è®¡ç®—MixupæŸå¤±
        loss = mixup.mixup_criterion(criterion, output, y_a, y_b, lam)
        
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(train_loader)
```

### 5.3 CutMix

#### å®ç°
```python
class CutMix:
    def __init__(self, alpha=1.0):
        self.alpha = alpha
    
    def __call__(self, batch_x, batch_y):
        if self.alpha > 0:
            lam = torch.distributions.Beta(self.alpha, self.alpha).sample()
        else:
            lam = 1
        
        batch_size = batch_x.size(0)
        index = torch.randperm(batch_size)
        
        # ç”Ÿæˆè£å‰ªåŒºåŸŸ
        W = batch_x.size(3)
        H = batch_x.size(2)
        
        cut_rat = torch.sqrt(1. - lam)
        cut_w = (W * cut_rat).int()
        cut_h = (H * cut_rat).int()
        
        # éšæœºé€‰æ‹©è£å‰ªä½ç½®
        cx = torch.randint(0, W, (1,))
        cy = torch.randint(0, H, (1,))
        
        bbx1 = torch.clamp(cx - cut_w // 2, 0, W)
        bby1 = torch.clamp(cy - cut_h // 2, 0, H)
        bbx2 = torch.clamp(cx + cut_w // 2, 0, W)
        bby2 = torch.clamp(cy + cut_h // 2, 0, H)
        
        # åˆ›å»ºæ··åˆå›¾åƒ
        mixed_x = batch_x.clone()
        mixed_x[:, :, bby1:bby2, bbx1:bbx2] = batch_x[index, :, bby1:bby2, bbx1:bbx2]
        
        # è°ƒæ•´Î»åŸºäºå®é™…è£å‰ªåŒºåŸŸ
        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (batch_x.size(2) * batch_x.size(3)))
        
        return mixed_x, batch_y, batch_y[index], lam

# ç»„åˆMixupå’ŒCutMix
class MixupCutmix:
    def __init__(self, mixup_alpha=1.0, cutmix_alpha=1.0, prob=0.5):
        self.mixup = Mixup(mixup_alpha)
        self.cutmix = CutMix(cutmix_alpha)
        self.prob = prob
    
    def __call__(self, batch_x, batch_y):
        if torch.rand(1) < self.prob:
            return self.mixup(batch_x, batch_y)
        else:
            return self.cutmix(batch_x, batch_y)
```

---

## âš¡ å…­ã€é«˜çº§æ­£åˆ™åŒ–æŠ€æœ¯

### 6.1 Spectral Normalization

#### åŸç†ä¸å®ç°
```python
class SpectralNorm:
    def __init__(self, module, name='weight', n_power_iterations=1, dim=0):
        self.module = module
        self.name = name
        self.dim = dim
        
        if n_power_iterations <= 0:
            raise ValueError('Expected n_power_iterations to be positive, but got: {}'.format(n_power_iterations))
        self.n_power_iterations = n_power_iterations
        
        weight = getattr(module, name)
        
        h, w = weight.size(dim), weight.view(weight.size(dim), -1).size(1)
        
        # åˆå§‹åŒ– u å‘é‡
        u = torch.randn(h).normal_(0, 1)
        u = u / torch.norm(u)
        
        self.register_buffer('weight_u', u)
    
    def compute_weight(self, do_power_iteration):
        weight = getattr(self.module, self.name + '_orig')
        u = self.weight_u
        
        if do_power_iteration:
            with torch.no_grad():
                for _ in range(self.n_power_iterations):
                    v = torch.mv(weight.view(weight.size(0), -1).t(), u)
                    v = v / torch.norm(v)
                    u = torch.mv(weight.view(weight.size(0), -1), v)
                    u = u / torch.norm(u)
        
        sigma = torch.dot(u, torch.mv(weight.view(weight.size(0), -1), v))
        weight = weight / sigma
        
        return weight
    
    def register_buffer(self, name, tensor):
        self.module.register_buffer(name, tensor)
    
    def __call__(self, module, inputs):
        weight = self.compute_weight(do_power_iteration=module.training)
        setattr(module, self.name, weight)

def spectral_norm(module, name='weight', n_power_iterations=1, dim=0):
    """æ·»åŠ spectral normalizationåˆ°æ¨¡å—"""
    SpectralNorm.apply(module, name, n_power_iterations, dim)
    return module
```

### 6.2 å™ªå£°æ³¨å…¥ (Noise Injection)

#### å®ç°
```python
class NoiseInjection:
    def __init__(self, noise_type='gaussian', noise_std=0.1):
        self.noise_type = noise_type
        self.noise_std = noise_std
    
    def add_gaussian_noise(self, x):
        """æ·»åŠ é«˜æ–¯å™ªå£°"""
        noise = torch.randn_like(x) * self.noise_std
        return x + noise
    
    def add_uniform_noise(self, x):
        """æ·»åŠ å‡åŒ€å™ªå£°"""
        noise = (torch.rand_like(x) - 0.5) * 2 * self.noise_std
        return x + noise
    
    def add_salt_pepper_noise(self, x, salt_prob=0.05, pepper_prob=0.05):
        """æ·»åŠ æ¤’ç›å™ªå£°"""
        noisy = x.clone()
        
        # ç›å™ªå£°ï¼ˆç™½ç‚¹ï¼‰
        salt = torch.rand_like(x) < salt_prob
        noisy[salt] = 1.0
        
        # èƒ¡æ¤’å™ªå£°ï¼ˆé»‘ç‚¹ï¼‰
        pepper = torch.rand_like(x) < pepper_prob
        noisy[pepper] = 0.0
        
        return noisy
    
    def __call__(self, x):
        if not torch.is_tensor(x):
            x = torch.tensor(x, dtype=torch.float32)
            
        if self.noise_type == 'gaussian':
            return self.add_gaussian_noise(x)
        elif self.noise_type == 'uniform':
            return self.add_uniform_noise(x)
        elif self.noise_type == 'salt_pepper':
            return self.add_salt_pepper_noise(x)
        else:
            raise ValueError(f"Unknown noise type: {self.noise_type}")

# è‡ªé€‚åº”å™ªå£°è°ƒåº¦
class AdaptiveNoiseScheduler:
    def __init__(self, initial_std=0.1, decay_rate=0.95, min_std=0.01):
        self.initial_std = initial_std
        self.current_std = initial_std
        self.decay_rate = decay_rate
        self.min_std = min_std
        self.epoch = 0
    
    def step(self, val_loss=None, train_loss=None):
        """æ ¹æ®è®­ç»ƒè¿›åº¦è°ƒæ•´å™ªå£°å¼ºåº¦"""
        self.epoch += 1
        
        # åŸºäºepochçš„è¡°å‡
        self.current_std = max(
            self.initial_std * (self.decay_rate ** self.epoch),
            self.min_std
        )
        
        # å¦‚æœæä¾›äº†æŸå¤±ä¿¡æ¯ï¼Œæ ¹æ®è¿‡æ‹Ÿåˆç¨‹åº¦è°ƒæ•´
        if val_loss is not None and train_loss is not None:
            overfitting_ratio = val_loss / train_loss
            if overfitting_ratio > 1.2:  # è¿‡æ‹Ÿåˆï¼Œå¢åŠ å™ªå£°
                self.current_std = min(self.current_std * 1.1, self.initial_std)
        
        return self.current_std
    
    def get_noise_injector(self):
        return NoiseInjection(noise_type='gaussian', noise_std=self.current_std)
```

---

## ğŸ“‹ ä¸ƒã€æ­£åˆ™åŒ–æŠ€æœ¯é€‰æ‹©ä¸ç»„åˆ

### 7.1 æ­£åˆ™åŒ–æŠ€æœ¯é€‰æ‹©çŸ©é˜µ

| åœºæ™¯ | æ¨èæŠ€æœ¯ | ç»„åˆç­–ç•¥ | æ³¨æ„äº‹é¡¹ |
|------|---------|---------|---------|
| **å°æ•°æ®é›†** | Dropout + Data Augmentation | å¼ºæ­£åˆ™åŒ–ç»„åˆ | é¿å…æ¬ æ‹Ÿåˆ |
| **å¤§æ•°æ®é›†** | BatchNorm + Weight Decay | è½»é‡æ­£åˆ™åŒ– | é‡ç‚¹å…³æ³¨è®­ç»ƒæ•ˆç‡ |
| **CNN** | BatchNorm + Dropout + Augmentation | å±‚æ¬¡åŒ–æ­£åˆ™åŒ– | BNåœ¨å·ç§¯å±‚ï¼ŒDropoutåœ¨å…¨è¿æ¥å±‚ |
| **RNN/LSTM** | Dropout + Gradient Clipping | ä¸“é—¨é’ˆå¯¹åºåˆ—æ¨¡å‹ | æ³¨æ„Dropoutä½ç½® |
| **Transformer** | LayerNorm + Dropout + Label Smoothing | ç°ä»£æ¶æ„æ ‡é… | æ³¨æ„Pre-Norm vs Post-Norm |
| **ç”Ÿæˆæ¨¡å‹** | Spectral Norm + Noise Injection | ç¨³å®šè®­ç»ƒ | å¹³è¡¡ç”Ÿæˆè´¨é‡å’Œç¨³å®šæ€§ |

### 7.2 è‡ªåŠ¨æ­£åˆ™åŒ–è°ƒä¼˜å™¨

```python
class AutoRegularizer:
    def __init__(self, model, config=None):
        self.model = model
        self.config = config or self.get_default_config()
        self.regularizers = {}
        self.performance_history = []
        
    def get_default_config(self):
        return {
            'dropout': {'enabled': True, 'initial_p': 0.5, 'adaptive': True},
            'weight_decay': {'enabled': True, 'initial_lambda': 0.01, 'adaptive': True},
            'batch_norm': {'enabled': True, 'momentum': 0.1},
            'label_smoothing': {'enabled': False, 'smoothing': 0.1},
            'data_augmentation': {'enabled': True, 'strength': 'medium'}
        }
    
    def setup_regularizers(self):
        """æ ¹æ®é…ç½®è®¾ç½®æ­£åˆ™åŒ–å™¨"""
        
        # Dropout
        if self.config['dropout']['enabled']:
            self.regularizers['dropout'] = AdaptiveDropout(
                initial_p=self.config['dropout']['initial_p']
            )
        
        # Weight Decay
        if self.config['weight_decay']['enabled']:
            self.regularizers['weight_decay'] = self.config['weight_decay']['initial_lambda']
        
        # Data Augmentation
        if self.config['data_augmentation']['enabled']:
            self.regularizers['augmentation'] = ImageAugmentation()
        
        # Label Smoothing
        if self.config['label_smoothing']['enabled']:
            self.regularizers['label_smoothing'] = LabelSmoothingLoss(
                num_classes=self.model.num_classes,
                smoothing=self.config['label_smoothing']['smoothing']
            )
    
    def update_regularization(self, epoch, train_loss, val_loss, val_acc):
        """æ ¹æ®è®­ç»ƒè¿›åº¦åŠ¨æ€è°ƒæ•´æ­£åˆ™åŒ–å¼ºåº¦"""
        
        performance = {
            'epoch': epoch,
            'train_loss': train_loss,
            'val_loss': val_loss,
            'val_acc': val_acc,
            'overfitting_ratio': val_loss / train_loss if train_loss > 0 else 1.0
        }
        
        self.performance_history.append(performance)
        
        # è‡ªé€‚åº”è°ƒæ•´ç­–ç•¥
        if len(self.performance_history) >= 5:
            recent_performance = self.performance_history[-5:]
            
            # æ£€æµ‹è¿‡æ‹Ÿåˆè¶‹åŠ¿
            overfitting_trend = [p['overfitting_ratio'] for p in recent_performance]
            is_overfitting = sum(overfitting_trend) / len(overfitting_trend) > 1.2
            
            # æ£€æµ‹æ¬ æ‹Ÿåˆ
            val_acc_trend = [p['val_acc'] for p in recent_performance]
            is_underfitting = max(val_acc_trend) - min(val_acc_trend) < 0.01
            
            if is_overfitting:
                self.increase_regularization()
            elif is_underfitting:
                self.decrease_regularization()
    
    def increase_regularization(self):
        """å¢å¼ºæ­£åˆ™åŒ–"""
        if 'dropout' in self.regularizers:
            current_p = self.regularizers['dropout'].p
            self.regularizers['dropout'].p = min(current_p * 1.1, 0.8)
            print(f"Increased dropout to {self.regularizers['dropout'].p:.3f}")
        
        if 'weight_decay' in self.regularizers:
            self.regularizers['weight_decay'] *= 1.2
            print(f"Increased weight decay to {self.regularizers['weight_decay']:.6f}")
    
    def decrease_regularization(self):
        """å‡å¼±æ­£åˆ™åŒ–"""
        if 'dropout' in self.regularizers:
            current_p = self.regularizers['dropout'].p
            self.regularizers['dropout'].p = max(current_p * 0.9, 0.1)
            print(f"Decreased dropout to {self.regularizers['dropout'].p:.3f}")
        
        if 'weight_decay' in self.regularizers:
            self.regularizers['weight_decay'] *= 0.9
            print(f"Decreased weight decay to {self.regularizers['weight_decay']:.6f}")

# æ­£åˆ™åŒ–æ•ˆæœè¯„ä¼°å™¨
class RegularizationEvaluator:
    def __init__(self):
        self.metrics = {}
    
    def evaluate_regularization_impact(self, baseline_model, regularized_model, test_loader):
        """è¯„ä¼°æ­£åˆ™åŒ–çš„å½±å“"""
        
        # è¯„ä¼°åŸºçº¿æ¨¡å‹
        baseline_metrics = self.evaluate_model(baseline_model, test_loader)
        
        # è¯„ä¼°æ­£åˆ™åŒ–æ¨¡å‹
        regularized_metrics = self.evaluate_model(regularized_model, test_loader)
        
        # è®¡ç®—æ”¹è¿›ç¨‹åº¦
        improvement = {
            'accuracy_gain': regularized_metrics['accuracy'] - baseline_metrics['accuracy'],
            'loss_reduction': baseline_metrics['loss'] - regularized_metrics['loss'],
            'calibration_improvement': regularized_metrics['calibration'] - baseline_metrics['calibration']
        }
        
        return {
            'baseline': baseline_metrics,
            'regularized': regularized_metrics,
            'improvement': improvement
        }
    
    def evaluate_model(self, model, test_loader):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        model.eval()
        total_loss = 0
        correct = 0
        total = 0
        predictions = []
        targets = []
        
        with torch.no_grad():
            for data, target in test_loader:
                output = model(data)
                loss = F.cross_entropy(output, target)
                total_loss += loss.item()
                
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
                
                # æ”¶é›†é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾ç”¨äºæ ¡å‡†è¯„ä¼°
                predictions.extend(F.softmax(output, dim=1).cpu().numpy())
                targets.extend(target.cpu().numpy())
        
        accuracy = correct / total
        avg_loss = total_loss / len(test_loader)
        calibration = self.calculate_calibration_error(predictions, targets)
        
        return {
            'accuracy': accuracy,
            'loss': avg_loss,
            'calibration': calibration
        }
    
    def calculate_calibration_error(self, predictions, targets):
        """è®¡ç®—æ ¡å‡†è¯¯å·®ï¼ˆECEï¼‰"""
        import numpy as np
        
        predictions = np.array(predictions)
        targets = np.array(targets)
        
        # å°†é¢„æµ‹æ¦‚ç‡åˆ†æˆbins
        n_bins = 10
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        bin_lowers = bin_boundaries[:-1]
        bin_uppers = bin_boundaries[1:]
        
        max_probs = np.max(predictions, axis=1)
        pred_labels = np.argmax(predictions, axis=1)
        accuracies = pred_labels == targets
        
        ece = 0
        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
            in_bin = (max_probs > bin_lower) & (max_probs <= bin_upper)
            prop_in_bin = in_bin.mean()
            
            if prop_in_bin > 0:
                accuracy_in_bin = accuracies[in_bin].mean()
                avg_confidence_in_bin = max_probs[in_bin].mean()
                ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
        
        return ece
```

---

## ğŸ¯ å…«ã€æœ€ä½³å®è·µä¸å»ºè®®

### 8.1 æ­£åˆ™åŒ–ç­–ç•¥æ¸…å•

**âœ… åŸºç¡€æ­£åˆ™åŒ–ï¼ˆå¿…é¡»ï¼‰**ï¼š
- [ ] ä½¿ç”¨é€‚å½“çš„æƒé‡è¡°å‡ï¼ˆ1e-4åˆ°1e-2ï¼‰
- [ ] åœ¨å…¨è¿æ¥å±‚ä½¿ç”¨Dropoutï¼ˆ0.3åˆ°0.7ï¼‰
- [ ] åº”ç”¨BatchNormæˆ–LayerNorm
- [ ] å®æ–½æ—©åœç­–ç•¥

**âœ… ä¸­çº§æ­£åˆ™åŒ–ï¼ˆæ¨èï¼‰**ï¼š
- [ ] æ•°æ®å¢å¼ºï¼ˆé’ˆå¯¹å…·ä½“ä»»åŠ¡ï¼‰
- [ ] æ ‡ç­¾å¹³æ»‘ï¼ˆå¤šåˆ†ç±»ä»»åŠ¡ï¼‰
- [ ] æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
- [ ] å­¦ä¹ ç‡è°ƒåº¦

**âœ… é«˜çº§æ­£åˆ™åŒ–ï¼ˆå¯é€‰ï¼‰**ï¼š
- [ ] Mixup/CutMixï¼ˆå›¾åƒä»»åŠ¡ï¼‰
- [ ] DropPathï¼ˆæ·±åº¦ç½‘ç»œï¼‰
- [ ] è°±å½’ä¸€åŒ–ï¼ˆGANè®­ç»ƒï¼‰
- [ ] å™ªå£°æ³¨å…¥

### 8.2 è°ƒè¯•ä¸ç›‘æ§å·¥å…·

```python
class RegularizationMonitor:
    def __init__(self):
        self.metrics = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': [],
            'gradient_norms': [],
            'weight_norms': []
        }
    
    def log_epoch(self, epoch, train_loss, val_loss, train_acc, val_acc, model):
        """è®°å½•æ¯ä¸ªepochçš„æŒ‡æ ‡"""
        self.metrics['train_loss'].append(train_loss)
        self.metrics['val_loss'].append(val_loss)
        self.metrics['train_acc'].append(train_acc)
        self.metrics['val_acc'].append(val_acc)
        
        # è®¡ç®—æ¢¯åº¦èŒƒæ•°
        total_grad_norm = 0
        for param in model.parameters():
            if param.grad is not None:
                total_grad_norm += param.grad.data.norm(2).item() ** 2
        self.metrics['gradient_norms'].append(total_grad_norm ** 0.5)
        
        # è®¡ç®—æƒé‡èŒƒæ•°
        total_weight_norm = 0
        for param in model.parameters():
            total_weight_norm += param.data.norm(2).item() ** 2
        self.metrics['weight_norms'].append(total_weight_norm ** 0.5)
    
    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        
        # æŸå¤±æ›²çº¿
        axes[0, 0].plot(self.metrics['train_loss'], label='Train Loss')
        axes[0, 0].plot(self.metrics['val_loss'], label='Val Loss')
        axes[0, 0].set_title('Loss Curves')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # å‡†ç¡®ç‡æ›²çº¿
        axes[0, 1].plot(self.metrics['train_acc'], label='Train Acc')
        axes[0, 1].plot(self.metrics['val_acc'], label='Val Acc')
        axes[0, 1].set_title('Accuracy Curves')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # æ³›åŒ–å·®è·
        gap = [v - t for t, v in zip(self.metrics['train_loss'], self.metrics['val_loss'])]
        axes[0, 2].plot(gap, label='Generalization Gap')
        axes[0, 2].set_title('Train-Val Loss Gap')
        axes[0, 2].legend()
        axes[0, 2].grid(True)
        
        # æ¢¯åº¦èŒƒæ•°
        axes[1, 0].plot(self.metrics['gradient_norms'])
        axes[1, 0].set_title('Gradient Norms')
        axes[1, 0].grid(True)
        
        # æƒé‡èŒƒæ•°
        axes[1, 1].plot(self.metrics['weight_norms'])
        axes[1, 1].set_title('Weight Norms')
        axes[1, 1].grid(True)
        
        # è¿‡æ‹Ÿåˆæ£€æµ‹
        if len(self.metrics['val_loss']) > 10:
            recent_trend = np.polyfit(range(len(self.metrics['val_loss'][-10:])), 
                                    self.metrics['val_loss'][-10:], 1)[0]
            if recent_trend > 0:
                axes[1, 2].text(0.5, 0.5, 'Potential Overfitting\nDetected', 
                               ha='center', va='center', transform=axes[1, 2].transAxes,
                               fontsize=16, color='red')
        
        axes[1, 2].set_title('Overfitting Detection')
        
        plt.tight_layout()
        plt.show()
    
    def get_regularization_recommendations(self):
        """åŸºäºç›‘æ§æŒ‡æ ‡æä¾›æ­£åˆ™åŒ–å»ºè®®"""
        recommendations = []
        
        if len(self.metrics['val_loss']) > 5:
            # æ£€æŸ¥è¿‡æ‹Ÿåˆ
            recent_train_loss = np.mean(self.metrics['train_loss'][-5:])
            recent_val_loss = np.mean(self.metrics['val_loss'][-5:])
            
            if recent_val_loss / recent_train_loss > 1.3:
                recommendations.append("æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆï¼Œå»ºè®®å¢åŠ æ­£åˆ™åŒ–å¼ºåº¦")
            elif recent_val_loss / recent_train_loss < 1.05:
                recommendations.append("å¯èƒ½å­˜åœ¨æ¬ æ‹Ÿåˆï¼Œè€ƒè™‘å‡å°‘æ­£åˆ™åŒ–")
            
            # æ£€æŸ¥è®­ç»ƒç¨³å®šæ€§
            val_loss_std = np.std(self.metrics['val_loss'][-10:]) if len(self.metrics['val_loss']) > 10 else 0
            if val_loss_std > 0.1:
                recommendations.append("è®­ç»ƒä¸ç¨³å®šï¼Œå»ºè®®æ·»åŠ BatchNormæˆ–é™ä½å­¦ä¹ ç‡")
            
            # æ£€æŸ¥æ¢¯åº¦å¥åº·çŠ¶å†µ
            if len(self.metrics['gradient_norms']) > 0:
                recent_grad_norm = self.metrics['gradient_norms'][-1]
                if recent_grad_norm > 10:
                    recommendations.append("æ¢¯åº¦è¿‡å¤§ï¼Œå»ºè®®æ·»åŠ æ¢¯åº¦è£å‰ª")
                elif recent_grad_norm < 1e-5:
                    recommendations.append("æ¢¯åº¦è¿‡å°ï¼Œå¯èƒ½å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜")
        
        return recommendations if recommendations else ["å½“å‰è®­ç»ƒçŠ¶æ€è‰¯å¥½"]
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- **é‡å­ä¼˜åŒ–**: [[K1-åŸºç¡€ç†è®ºä¸æ¦‚å¿µ/è®¡ç®—åŸºç¡€/é‡å­è®¡ç®—é¿å…å±€éƒ¨æœ€ä¼˜ï¼šåŸç†ã€æŒ‘æˆ˜ä¸AIåº”ç”¨å‰æ²¿|é‡å­è®¡ç®—é¿å…å±€éƒ¨æœ€ä¼˜ï¼šåŸç†ã€æŒ‘æˆ˜ä¸AIåº”ç”¨å‰æ²¿]]
- **Losså‡½æ•°è°ƒä¼˜**: [[K2-æŠ€æœ¯æ–¹æ³•ä¸å®ç°/è®­ç»ƒæŠ€æœ¯/Losså‡½æ•°ä¸æ¨¡å‹è°ƒä¼˜å…¨é¢æŒ‡å—|Losså‡½æ•°ä¸æ¨¡å‹è°ƒä¼˜å…¨é¢æŒ‡å—]]
- **æŸå¤±å‡½æ•°è¯¦è§£**: [[K2-æŠ€æœ¯æ–¹æ³•ä¸å®ç°/è®­ç»ƒæŠ€æœ¯/æŸå¤±å‡½æ•°ç±»å‹å…¨è§£æï¼šä»åŸºç¡€åˆ°é«˜çº§åº”ç”¨|æŸå¤±å‡½æ•°ç±»å‹å…¨è§£æï¼šä»åŸºç¡€åˆ°é«˜çº§åº”ç”¨]]
- **ä¼˜åŒ–å™¨ç®—æ³•**: [[K2-æŠ€æœ¯æ–¹æ³•ä¸å®ç°/ä¼˜åŒ–æ–¹æ³•/æ·±åº¦å­¦ä¹ ä¼˜åŒ–å™¨ç®—æ³•å¯¹æ¯”åˆ†æ|æ·±åº¦å­¦ä¹ ä¼˜åŒ–å™¨ç®—æ³•å¯¹æ¯”åˆ†æ]]

---

**æ›´æ–°æ—¶é—´**: 2025å¹´1æœˆ  
**ç»´æŠ¤è€…**: AIçŸ¥è¯†åº“å›¢é˜Ÿ  
**éš¾åº¦è¯„çº§**: â­â­â­â­ (éœ€è¦æ·±å…¥ç†è§£æ·±åº¦å­¦ä¹ ç†è®ºå’Œä¸°å¯Œçš„å®è·µç»éªŒ)