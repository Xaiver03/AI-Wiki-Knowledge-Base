# æ·±åº¦å­¦ä¹ ä¼˜åŒ–å™¨ç®—æ³•å¯¹æ¯”åˆ†æ

> **æ ‡ç­¾**: ä¼˜åŒ–ç®—æ³• | æ¢¯åº¦ä¸‹é™ | è‡ªé€‚åº”å­¦ä¹ ç‡ | æ¨¡å‹è®­ç»ƒ  
> **é€‚ç”¨åœºæ™¯**: æ¨¡å‹è®­ç»ƒä¼˜åŒ–ã€ç®—æ³•é€‰æ‹©ã€è¶…å‚æ•°è°ƒä¼˜  
> **éš¾åº¦çº§åˆ«**: â­â­â­â­
> **å…³è”**ï¼š[[K1-åŸºç¡€ç†è®ºä¸æ¦‚å¿µ/æ ¸å¿ƒæ¦‚å¿µ/æŸå¤±å‡½æ•°ä¸è®­ç»ƒè°ƒä¼˜æœ¯è¯­åè¯åº“|æœ¯è¯­åè¯åº“ï¼ˆå¤§ç™½è¯å¯¹ç…§ï¼‰]]

## ğŸ“‹ æ¦‚è¿°

ä¼˜åŒ–å™¨ï¼ˆOptimizerï¼‰æ˜¯æ·±åº¦å­¦ä¹ è®­ç»ƒçš„æ ¸å¿ƒå¼•æ“ï¼Œè´Ÿè´£æ ¹æ®æŸå¤±å‡½æ•°çš„æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ã€‚ä¸åŒçš„ä¼˜åŒ–å™¨é‡‡ç”¨ä¸åŒçš„ç­–ç•¥æ¥åŠ é€Ÿæ”¶æ•›ã€é¿å…å±€éƒ¨æœ€ä¼˜ã€æé«˜è®­ç»ƒç¨³å®šæ€§ã€‚æœ¬æ–‡å°†æ·±å…¥åˆ†æä¸»æµä¼˜åŒ–å™¨çš„åŸç†ã€ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯ã€‚

---

## ğŸ¯ ä¸€ã€ä¼˜åŒ–å™¨åŸºç¡€ç†è®º

### 1.1 æ¢¯åº¦ä¸‹é™åŸºæœ¬åŸç†

**æ ¸å¿ƒæ€æƒ³**ï¼šæ²¿ç€æŸå¤±å‡½æ•°æ¢¯åº¦çš„åæ–¹å‘æ›´æ–°å‚æ•°ï¼Œé€æ­¥æ‰¾åˆ°æœ€ä¼˜è§£ã€‚

**åŸºæœ¬å…¬å¼**ï¼š
```
Î¸(t+1) = Î¸(t) - Î·âˆ‡J(Î¸(t))

å…¶ä¸­ï¼š
- Î¸ï¼šæ¨¡å‹å‚æ•°
- Î·ï¼šå­¦ä¹ ç‡
- âˆ‡J(Î¸)ï¼šæŸå¤±å‡½æ•°å…³äºå‚æ•°çš„æ¢¯åº¦
- tï¼šæ—¶é—´æ­¥ï¼ˆè¿­ä»£æ¬¡æ•°ï¼‰
```

### 1.2 ä¼˜åŒ–å™¨è®¾è®¡è€ƒè™‘å› ç´ 

| è€ƒè™‘å› ç´  | è¯´æ˜ | å½±å“ |
|----------|------|------|
| **æ”¶æ•›é€Ÿåº¦** | è¾¾åˆ°æœ€ä¼˜è§£çš„å¿«æ…¢ | è®­ç»ƒæ—¶é—´ |
| **æ”¶æ•›ç¨³å®šæ€§** | è®­ç»ƒè¿‡ç¨‹æ˜¯å¦å¹³æ»‘ | è®­ç»ƒæˆåŠŸç‡ |
| **æ³›åŒ–èƒ½åŠ›** | é¿å…è¿‡æ‹Ÿåˆçš„èƒ½åŠ› | æ¨¡å‹æ€§èƒ½ |
| **å†…å­˜ä½¿ç”¨** | é¢å¤–çŠ¶æ€å­˜å‚¨éœ€æ±‚ | èµ„æºæ¶ˆè€— |
| **è¶…å‚æ•°æ•æ„Ÿæ€§** | å¯¹å­¦ä¹ ç‡ç­‰å‚æ•°çš„ä¾èµ– | è°ƒå‚éš¾åº¦ |

---

## ğŸ”§ äºŒã€ä¸»æµä¼˜åŒ–å™¨è¯¦ç»†åˆ†æ

### 2.1 SGD (Stochastic Gradient Descent)

#### åŸºæœ¬åŸç†
æœ€åŸºç¡€çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œæ¯æ¬¡ä½¿ç”¨ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®è®¡ç®—æ¢¯åº¦ã€‚

#### æ•°å­¦å…¬å¼
```
Î¸(t+1) = Î¸(t) - Î·âˆ‡J(Î¸(t))
```

#### å¸¦åŠ¨é‡çš„SGD (SGD with Momentum)
```
v(t+1) = Î²v(t) + Î·âˆ‡J(Î¸(t))
Î¸(t+1) = Î¸(t) - v(t+1)

å…¶ä¸­ï¼šÎ²ä¸ºåŠ¨é‡ç³»æ•°ï¼Œé€šå¸¸å–0.9
```

#### ä»£ç å®ç°
```python
class SGD:
    def __init__(self, params, lr=0.01, momentum=0.9):
        self.params = params
        self.lr = lr
        self.momentum = momentum
        self.velocity = [torch.zeros_like(p) for p in params]
    
    def step(self, gradients):
        for i, (param, grad) in enumerate(zip(self.params, gradients)):
            self.velocity[i] = self.momentum * self.velocity[i] + self.lr * grad
            param.data -= self.velocity[i]
```

#### ä¼˜åŠ¿ä¸åŠ£åŠ¿
**âœ… ä¼˜åŠ¿**ï¼š
- ç®€å•å¯é ï¼Œç†è®ºåŸºç¡€æ‰å®
- å†…å­˜å ç”¨å°‘
- æ³›åŒ–èƒ½åŠ›å¼ºï¼Œä¸å®¹æ˜“è¿‡æ‹Ÿåˆ
- åœ¨å¤§æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½

**âŒ åŠ£åŠ¿**ï¼š
- æ”¶æ•›é€Ÿåº¦ç›¸å¯¹è¾ƒæ…¢
- å¯¹å­¦ä¹ ç‡æ•æ„Ÿï¼Œéœ€è¦ç²¾å¿ƒè°ƒå‚
- åœ¨éç‚¹å¤„å®¹æ˜“é™·å…¥æŒ¯è¡
- å¤„ç†ç¨€ç–æ¢¯åº¦æ•ˆç‡ä½

#### é€‚ç”¨åœºæ™¯
- å¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒ
- è®¡ç®—èµ„æºå—é™çš„ç¯å¢ƒ
- éœ€è¦è‰¯å¥½æ³›åŒ–æ€§èƒ½çš„åœºæ™¯
- ä¼ ç»ŸCNNã€RNNè®­ç»ƒ

### 2.2 Adam (Adaptive Moment Estimation)

#### åŸºæœ¬åŸç†
ç»“åˆäº†åŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œç»´æŠ¤æ¯ä¸ªå‚æ•°çš„ä¸€é˜¶çŸ©ä¼°è®¡ï¼ˆåŠ¨é‡ï¼‰å’ŒäºŒé˜¶çŸ©ä¼°è®¡ï¼ˆæ¢¯åº¦å¹³æ–¹çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰ã€‚

#### æ•°å­¦å…¬å¼
```
m(t) = Î²1*m(t-1) + (1-Î²1)*g(t)          # ä¸€é˜¶çŸ©ä¼°è®¡
v(t) = Î²2*v(t-1) + (1-Î²2)*g(t)Â²         # äºŒé˜¶çŸ©ä¼°è®¡

mÌ‚(t) = m(t) / (1-Î²1^t)                  # åå·®ä¿®æ­£
vÌ‚(t) = v(t) / (1-Î²2^t)                  # åå·®ä¿®æ­£

Î¸(t+1) = Î¸(t) - Î· * mÌ‚(t) / (âˆšvÌ‚(t) + Îµ)

é»˜è®¤å‚æ•°ï¼šÎ²1=0.9, Î²2=0.999, Îµ=1e-8
```

#### ä»£ç å®ç°
```python
class Adam:
    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        self.params = params
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.m = [torch.zeros_like(p) for p in params]
        self.v = [torch.zeros_like(p) for p in params]
        self.step_count = 0
    
    def step(self, gradients):
        self.step_count += 1
        
        for i, (param, grad) in enumerate(zip(self.params, gradients)):
            # æ›´æ–°åŠ¨é‡
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad**2
            
            # åå·®ä¿®æ­£
            m_hat = self.m[i] / (1 - self.beta1**self.step_count)
            v_hat = self.v[i] / (1 - self.beta2**self.step_count)
            
            # å‚æ•°æ›´æ–°
            param.data -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)
```

#### ä¼˜åŠ¿ä¸åŠ£åŠ¿
**âœ… ä¼˜åŠ¿**ï¼š
- è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œå¯¹ä¸åŒå‚æ•°é‡‡ç”¨ä¸åŒçš„æ›´æ–°æ­¥é•¿
- å¯¹è¶…å‚æ•°ç›¸å¯¹ä¸æ•æ„Ÿ
- æ”¶æ•›é€Ÿåº¦å¿«ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒåˆæœŸ
- å¤„ç†ç¨€ç–æ¢¯åº¦æ•ˆæœå¥½
- å¹¿æ³›é€‚ç”¨äºå„ç§æ·±åº¦å­¦ä¹ ä»»åŠ¡

**âŒ åŠ£åŠ¿**ï¼š
- å†…å­˜å ç”¨è¾ƒå¤§ï¼ˆéœ€å­˜å‚¨ä¸€é˜¶å’ŒäºŒé˜¶çŸ©ï¼‰
- å¯èƒ½æ”¶æ•›åˆ°è¾ƒå·®çš„å±€éƒ¨æœ€ä¼˜
- åœ¨æŸäº›æƒ…å†µä¸‹æ³›åŒ–æ€§èƒ½ä¸å¦‚SGD
- å­¦ä¹ ç‡è¡°å‡å¯èƒ½å¯¼è‡´è¿‡æ—©åœæ­¢å­¦ä¹ 

#### é€‚ç”¨åœºæ™¯
- å¿«é€ŸåŸå‹å¼€å‘
- å¤æ‚æ¨¡å‹ï¼ˆTransformerã€GANç­‰ï¼‰
- ç¨€ç–æ•°æ®å¤„ç†
- éœ€è¦å¿«é€Ÿæ”¶æ•›çš„åœºæ™¯

### 2.3 AdamW (Adam with Weight Decay)

#### åŸºæœ¬åŸç†
åœ¨Adamçš„åŸºç¡€ä¸Šä¿®æ­£äº†æƒé‡è¡°å‡çš„å®ç°æ–¹å¼ï¼Œå°†æƒé‡è¡°å‡ä»æ¢¯åº¦ä¸­åˆ†ç¦»å‡ºæ¥ã€‚

#### æ•°å­¦å…¬å¼
```
m(t) = Î²1*m(t-1) + (1-Î²1)*g(t)
v(t) = Î²2*v(t-1) + (1-Î²2)*g(t)Â²

mÌ‚(t) = m(t) / (1-Î²1^t)
vÌ‚(t) = v(t) / (1-Î²2^t)

Î¸(t+1) = Î¸(t) - Î· * (mÌ‚(t) / (âˆšvÌ‚(t) + Îµ) + Î»*Î¸(t))

å…¶ä¸­Î»ä¸ºæƒé‡è¡°å‡ç³»æ•°
```

#### ä»£ç å®ç°
```python
class AdamW:
    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01):
        self.params = params
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.weight_decay = weight_decay
        self.m = [torch.zeros_like(p) for p in params]
        self.v = [torch.zeros_like(p) for p in params]
        self.step_count = 0
    
    def step(self, gradients):
        self.step_count += 1
        
        for i, (param, grad) in enumerate(zip(self.params, gradients)):
            # æƒé‡è¡°å‡
            param.data.mul_(1 - self.lr * self.weight_decay)
            
            # Adamæ›´æ–°
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad**2
            
            m_hat = self.m[i] / (1 - self.beta1**self.step_count)
            v_hat = self.v[i] / (1 - self.beta2**self.step_count)
            
            param.data -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)
```

#### ä¼˜åŠ¿
- ä¿®æ­£äº†Adamä¸­æƒé‡è¡°å‡çš„é—®é¢˜
- æ›´å¥½çš„æ­£åˆ™åŒ–æ•ˆæœ
- åœ¨Transformerç­‰æ¨¡å‹ä¸Šè¡¨ç°ä¼˜ç§€
- å¹³è¡¡äº†æ”¶æ•›é€Ÿåº¦å’Œæ³›åŒ–èƒ½åŠ›

### 2.4 RMSprop (Root Mean Square Propagation)

#### åŸºæœ¬åŸç†
ä½¿ç”¨æ¢¯åº¦å¹³æ–¹çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡æ¥è°ƒæ•´å­¦ä¹ ç‡ï¼Œè§£å†³Adagradå­¦ä¹ ç‡è¡°å‡è¿‡å¿«çš„é—®é¢˜ã€‚

#### æ•°å­¦å…¬å¼
```
v(t) = Î²*v(t-1) + (1-Î²)*g(t)Â²
Î¸(t+1) = Î¸(t) - Î·*g(t) / (âˆšv(t) + Îµ)

å…¶ä¸­Î²é€šå¸¸å–0.9
```

#### ä»£ç å®ç°
```python
class RMSprop:
    def __init__(self, params, lr=0.01, beta=0.9, eps=1e-8):
        self.params = params
        self.lr = lr
        self.beta = beta
        self.eps = eps
        self.v = [torch.zeros_like(p) for p in params]
    
    def step(self, gradients):
        for i, (param, grad) in enumerate(zip(self.params, gradients)):
            self.v[i] = self.beta * self.v[i] + (1 - self.beta) * grad**2
            param.data -= self.lr * grad / (torch.sqrt(self.v[i]) + self.eps)
```

#### ä¼˜åŠ¿ä¸åŠ£åŠ¿
**âœ… ä¼˜åŠ¿**ï¼š
- è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´
- é€‚åˆå¤„ç†éå¹³ç¨³æ•°æ®
- åœ¨RNNè®­ç»ƒä¸­è¡¨ç°è‰¯å¥½
- å†…å­˜å ç”¨é€‚ä¸­

**âŒ åŠ£åŠ¿**ï¼š
- ä»å¯èƒ½å‡ºç°å­¦ä¹ ç‡è¿‡åº¦è¡°å‡
- å¯¹è¶…å‚æ•°æ•æ„Ÿ
- æ”¶æ•›é€Ÿåº¦ä¸å¦‚Adam

### 2.5 Adagrad (Adaptive Gradient)

#### åŸºæœ¬åŸç†
ç´¯ç§¯å†å²æ¢¯åº¦çš„å¹³æ–¹å’Œæ¥è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡ï¼Œç»™é¢‘ç¹æ›´æ–°çš„å‚æ•°æ›´å°çš„å­¦ä¹ ç‡ã€‚

#### æ•°å­¦å…¬å¼
```
G(t) = G(t-1) + g(t)Â²                    # ç´¯ç§¯æ¢¯åº¦å¹³æ–¹
Î¸(t+1) = Î¸(t) - Î·*g(t) / (âˆšG(t) + Îµ)
```

#### ä¼˜åŠ¿ä¸åŠ£åŠ¿
**âœ… ä¼˜åŠ¿**ï¼š
- è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡
- å¯¹ç¨€ç–ç‰¹å¾å‹å¥½
- ç†è®ºåŸºç¡€æ‰å®

**âŒ åŠ£åŠ¿**ï¼š
- å­¦ä¹ ç‡å•è°ƒé€’å‡ï¼Œå¯èƒ½è¿‡æ—©åœæ­¢å­¦ä¹ 
- åœ¨æ·±åº¦å­¦ä¹ ä¸­å¾ˆå°‘ä½¿ç”¨

---

## ğŸ“Š ä¸‰ã€ä¼˜åŒ–å™¨æ€§èƒ½å¯¹æ¯”

### 3.1 æ”¶æ•›é€Ÿåº¦å¯¹æ¯”

```python
import matplotlib.pyplot as plt
import numpy as np

def compare_optimizers_convergence():
    """æ¨¡æ‹Ÿä¸åŒä¼˜åŒ–å™¨çš„æ”¶æ•›æ›²çº¿"""
    epochs = 100
    
    # æ¨¡æ‹ŸæŸå¤±æ›²çº¿
    sgd_loss = 2.0 * np.exp(-0.05 * np.arange(epochs)) + 0.1 + 0.05 * np.random.randn(epochs)
    adam_loss = 2.0 * np.exp(-0.12 * np.arange(epochs)) + 0.15 + 0.03 * np.random.randn(epochs)
    adamw_loss = 2.0 * np.exp(-0.10 * np.arange(epochs)) + 0.12 + 0.02 * np.random.randn(epochs)
    rmsprop_loss = 2.0 * np.exp(-0.08 * np.arange(epochs)) + 0.13 + 0.04 * np.random.randn(epochs)
    
    plt.figure(figsize=(12, 8))
    plt.plot(sgd_loss, label='SGD', linewidth=2)
    plt.plot(adam_loss, label='Adam', linewidth=2)
    plt.plot(adamw_loss, label='AdamW', linewidth=2)
    plt.plot(rmsprop_loss, label='RMSprop', linewidth=2)
    
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Optimizers Convergence Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
```

### 3.2 è¯¦ç»†æ€§èƒ½å¯¹æ¯”è¡¨

| ä¼˜åŒ–å™¨ | æ”¶æ•›é€Ÿåº¦ | å†…å­˜ä½¿ç”¨ | è¶…å‚æ•°æ•æ„Ÿæ€§ | æ³›åŒ–èƒ½åŠ› | ç¨³å®šæ€§ | é€‚ç”¨åœºæ™¯ |
|--------|----------|----------|-------------|----------|--------|----------|
| **SGD** | â­â­ | â­â­â­â­â­ | â­â­ | â­â­â­â­â­ | â­â­â­ | å¤§æ•°æ®é›†ã€CNN |
| **SGD+Momentum** | â­â­â­ | â­â­â­â­ | â­â­ | â­â­â­â­â­ | â­â­â­â­ | ä¼ ç»Ÿæ·±åº¦å­¦ä¹  |
| **Adam** | â­â­â­â­â­ | â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­ | å¿«é€ŸåŸå‹ã€å¤æ‚æ¨¡å‹ |
| **AdamW** | â­â­â­â­ | â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | Transformerã€ç°ä»£DL |
| **RMSprop** | â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­ | RNNã€éå¹³ç¨³æ•°æ® |
| **Adagrad** | â­â­ | â­â­â­ | â­â­â­ | â­â­ | â­â­ | ç¨€ç–æ•°æ® |

### 3.3 ä¸åŒä»»åŠ¡ç±»å‹çš„æ¨è

```python
def get_optimizer_recommendation(task_type, dataset_size, compute_budget):
    """æ ¹æ®ä»»åŠ¡ç‰¹ç‚¹æ¨èä¼˜åŒ–å™¨"""
    
    recommendations = {
        'image_classification': {
            'large_dataset': 'SGD with Momentum',
            'small_dataset': 'Adam',
            'limited_compute': 'SGD'
        },
        'nlp_transformer': {
            'any_size': 'AdamW',
            'limited_compute': 'Adam'
        },
        'rnn_sequence': {
            'any_size': 'RMSprop',
            'alternative': 'Adam'
        },
        'gan_training': {
            'discriminator': 'Adam',
            'generator': 'Adam'
        },
        'reinforcement_learning': {
            'policy_gradient': 'Adam',
            'value_function': 'RMSprop'
        }
    }
    
    if task_type in recommendations:
        if dataset_size == 'large':
            return recommendations[task_type].get('large_dataset', 'SGD')
        elif compute_budget == 'limited':
            return recommendations[task_type].get('limited_compute', 'Adam')
        else:
            return recommendations[task_type].get('any_size', 'Adam')
    else:
        return 'Adam (é€šç”¨é€‰æ‹©)'
```

---

## ğŸ› ï¸ å››ã€ä¼˜åŒ–å™¨é€‰æ‹©ç­–ç•¥

### 4.1 å†³ç­–æ ‘æŒ‡å—

```
å¼€å§‹è®­ç»ƒæ–°æ¨¡å‹
â”œâ”€â”€ æ˜¯å¦ä¸ºTransformeræ¶æ„ï¼Ÿ
â”‚   â”œâ”€â”€ æ˜¯ â†’ æ¨èAdamW (lr=1e-3~5e-4)
â”‚   â””â”€â”€ å¦ â†’ ç»§ç»­ä¸‹ä¸€æ­¥
â”œâ”€â”€ æ•°æ®é›†è§„æ¨¡å¦‚ä½•ï¼Ÿ
â”‚   â”œâ”€â”€ å¤§è§„æ¨¡(>1Mæ ·æœ¬) â†’ æ¨èSGD+Momentum (lr=1e-1~1e-2)
â”‚   â”œâ”€â”€ ä¸­è§„æ¨¡(10K~1M) â†’ æ¨èAdam (lr=1e-3~1e-4)
â”‚   â””â”€â”€ å°è§„æ¨¡(<10K) â†’ æ¨èAdam (lr=1e-3~1e-4)
â”œâ”€â”€ è®¡ç®—èµ„æºæ˜¯å¦å—é™ï¼Ÿ
â”‚   â”œâ”€â”€ æ˜¯ â†’ æ¨èSGD
â”‚   â””â”€â”€ å¦ â†’ æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©
â””â”€â”€ æ˜¯å¦éœ€è¦å¿«é€Ÿæ”¶æ•›ï¼Ÿ
    â”œâ”€â”€ æ˜¯ â†’ æ¨èAdam
    â””â”€â”€ å¦ â†’ æ¨èSGD+Momentum
```

### 4.2 è¶…å‚æ•°è®¾ç½®æŒ‡å—

#### SGDæ¨èè®¾ç½®
```python
# å›¾åƒåˆ†ç±»
optimizer = torch.optim.SGD(
    model.parameters(), 
    lr=0.1,                    # åˆå§‹å­¦ä¹ ç‡
    momentum=0.9,              # åŠ¨é‡ç³»æ•°
    weight_decay=1e-4,         # æƒé‡è¡°å‡
    nesterov=True              # NesterovåŠ¨é‡
)

# å­¦ä¹ ç‡è°ƒåº¦
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
```

#### Adamæ¨èè®¾ç½®
```python
# é€šç”¨è®¾ç½®
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=1e-3,                   # å­¦ä¹ ç‡
    betas=(0.9, 0.999),        # åŠ¨é‡å‚æ•°
    eps=1e-8,                  # æ•°å€¼ç¨³å®šæ€§
    weight_decay=1e-4          # æƒé‡è¡°å‡
)
```

#### AdamWæ¨èè®¾ç½®
```python
# Transformeræ¨¡å‹
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=5e-4,                   # è¾ƒå°çš„å­¦ä¹ ç‡
    betas=(0.9, 0.999),        # æ ‡å‡†åŠ¨é‡å‚æ•°
    eps=1e-8,
    weight_decay=0.01          # è¾ƒå¼ºçš„æƒé‡è¡°å‡
)

# é…åˆä½™å¼¦é€€ç«è°ƒåº¦å™¨
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
```

### 4.3 åŠ¨æ€ä¼˜åŒ–å™¨åˆ‡æ¢

```python
class AdaptiveOptimizerSwitcher:
    def __init__(self, model, initial_optimizer='adam'):
        self.model = model
        self.current_optimizer = initial_optimizer
        self.loss_history = []
        self.switch_patience = 20
        self.no_improvement_count = 0
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizers = {
            'adam': torch.optim.Adam(model.parameters(), lr=1e-3),
            'sgd': torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9),
            'adamw': torch.optim.AdamW(model.parameters(), lr=5e-4)
        }
    
    def step(self, loss):
        """æ ¹æ®losså˜åŒ–å†³å®šæ˜¯å¦åˆ‡æ¢ä¼˜åŒ–å™¨"""
        self.loss_history.append(loss)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢
        if len(self.loss_history) >= self.switch_patience:
            recent_improvement = self.loss_history[-self.switch_patience] - loss
            
            if recent_improvement < 0.001:  # å‡ ä¹æ²¡æœ‰æ”¹å–„
                self.no_improvement_count += 1
                
                if self.no_improvement_count >= 5:
                    self.switch_optimizer()
                    self.no_improvement_count = 0
            else:
                self.no_improvement_count = 0
    
    def switch_optimizer(self):
        """åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªä¼˜åŒ–å™¨"""
        switch_order = ['adam', 'adamw', 'sgd']
        current_idx = switch_order.index(self.current_optimizer)
        next_idx = (current_idx + 1) % len(switch_order)
        
        self.current_optimizer = switch_order[next_idx]
        print(f"åˆ‡æ¢åˆ°ä¼˜åŒ–å™¨: {self.current_optimizer}")
    
    def get_current_optimizer(self):
        return self.optimizers[self.current_optimizer]
```

---

## ğŸ“ˆ äº”ã€é«˜çº§ä¼˜åŒ–æŠ€æœ¯

### 5.1 å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥

#### æ­¥è¿›è¡°å‡ (Step Decay)
```python
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
```

#### ä½™å¼¦é€€ç« (Cosine Annealing)
```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
```

#### ä½™å¼¦é‡å¯ (Cosine Annealing with Warm Restarts)
```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer, T_0=10, T_mult=2
)
```

#### è‡ªå®šä¹‰Warmup + ä½™å¼¦è¡°å‡
```python
class WarmupCosineScheduler:
    def __init__(self, optimizer, warmup_epochs, total_epochs, base_lr, min_lr=1e-6):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs
        self.base_lr = base_lr
        self.min_lr = min_lr
    
    def step(self, epoch):
        if epoch < self.warmup_epochs:
            # Warmupé˜¶æ®µ
            lr = self.base_lr * epoch / self.warmup_epochs
        else:
            # ä½™å¼¦è¡°å‡é˜¶æ®µ
            progress = (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)
            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + math.cos(math.pi * progress))
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
```

### 5.2 æ¢¯åº¦å¤„ç†æŠ€æœ¯

#### æ¢¯åº¦è£å‰ª
```python
# æŒ‰èŒƒæ•°è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# æŒ‰å€¼è£å‰ª
torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)
```

#### æ¢¯åº¦ç´¯ç§¯
```python
def train_with_gradient_accumulation(model, dataloader, optimizer, accumulation_steps=4):
    model.train()
    optimizer.zero_grad()
    
    for i, (inputs, targets) in enumerate(dataloader):
        outputs = model(inputs)
        loss = criterion(outputs, targets) / accumulation_steps
        loss.backward()
        
        if (i + 1) % accumulation_steps == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            optimizer.zero_grad()
```

### 5.3 äºŒé˜¶ä¼˜åŒ–æ–¹æ³•

#### L-BFGS (Limited-memory BFGS)
```python
# é€‚ç”¨äºå°æ•°æ®é›†çš„äºŒé˜¶ä¼˜åŒ–
optimizer = torch.optim.LBFGS(model.parameters(), lr=1.0)

def closure():
    optimizer.zero_grad()
    output = model(input)
    loss = criterion(output, target)
    loss.backward()
    return loss

optimizer.step(closure)
```

---

## ğŸ¯ å…­ã€å®æˆ˜ä¼˜åŒ–ç­–ç•¥

### 6.1 ä¸åŒæ¨¡å‹æ¶æ„çš„ä¼˜åŒ–å™¨é€‰æ‹©

```python
class ModelSpecificOptimizer:
    @staticmethod
    def get_optimizer(model_type, model, **kwargs):
        """æ ¹æ®æ¨¡å‹ç±»å‹è¿”å›æœ€é€‚åˆçš„ä¼˜åŒ–å™¨é…ç½®"""
        
        configs = {
            'resnet': {
                'optimizer': torch.optim.SGD,
                'params': {
                    'lr': 0.1,
                    'momentum': 0.9,
                    'weight_decay': 1e-4,
                    'nesterov': True
                }
            },
            'transformer': {
                'optimizer': torch.optim.AdamW,
                'params': {
                    'lr': 5e-4,
                    'betas': (0.9, 0.999),
                    'weight_decay': 0.01,
                    'eps': 1e-8
                }
            },
            'lstm': {
                'optimizer': torch.optim.RMSprop,
                'params': {
                    'lr': 1e-3,
                    'alpha': 0.99,
                    'eps': 1e-8,
                    'weight_decay': 1e-4
                }
            },
            'gan_generator': {
                'optimizer': torch.optim.Adam,
                'params': {
                    'lr': 2e-4,
                    'betas': (0.5, 0.999)
                }
            },
            'gan_discriminator': {
                'optimizer': torch.optim.Adam,
                'params': {
                    'lr': 2e-4,
                    'betas': (0.5, 0.999)
                }
            }
        }
        
        if model_type in configs:
            config = configs[model_type]
            # åˆå¹¶ç”¨æˆ·æä¾›çš„å‚æ•°
            params = {**config['params'], **kwargs}
            return config['optimizer'](model.parameters(), **params)
        else:
            # é»˜è®¤é€‰æ‹©Adam
            return torch.optim.Adam(model.parameters(), lr=1e-3)
```

### 6.2 è‡ªé€‚åº”ä¼˜åŒ–å™¨é€‰æ‹©ç³»ç»Ÿ

```python
class SmartOptimizerSelector:
    def __init__(self, model, train_loader, val_loader, trial_epochs=5):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.trial_epochs = trial_epochs
        
    def evaluate_optimizer(self, optimizer_config):
        """è¯„ä¼°å•ä¸ªä¼˜åŒ–å™¨é…ç½®çš„æ€§èƒ½"""
        model_copy = copy.deepcopy(self.model)
        optimizer = optimizer_config['class'](
            model_copy.parameters(), 
            **optimizer_config['params']
        )
        
        train_losses = []
        val_losses = []
        
        for epoch in range(self.trial_epochs):
            # è®­ç»ƒ
            model_copy.train()
            train_loss = 0
            for batch_idx, (data, target) in enumerate(self.train_loader):
                optimizer.zero_grad()
                output = model_copy(data)
                loss = F.cross_entropy(output, target)
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
                
                if batch_idx > 10:  # åªç”¨å°‘é‡æ‰¹æ¬¡è¯„ä¼°
                    break
            
            train_losses.append(train_loss / min(len(self.train_loader), 11))
            
            # éªŒè¯
            model_copy.eval()
            val_loss = 0
            with torch.no_grad():
                for batch_idx, (data, target) in enumerate(self.val_loader):
                    output = model_copy(data)
                    val_loss += F.cross_entropy(output, target).item()
                    if batch_idx > 5:  # å°‘é‡éªŒè¯
                        break
            
            val_losses.append(val_loss / min(len(self.val_loader), 6))
        
        # è¯„ä¼°æŒ‡æ ‡ï¼šæœ€ç»ˆæŸå¤± + æ”¶æ•›é€Ÿåº¦ + ç¨³å®šæ€§
        final_train_loss = train_losses[-1]
        final_val_loss = val_losses[-1]
        convergence_speed = train_losses[0] - train_losses[-1]
        stability = -np.std(val_losses)  # è´Ÿæ ‡å‡†å·®ï¼Œè¶Šå¤§è¶Šç¨³å®š
        
        score = -final_val_loss + 0.3 * convergence_speed + 0.1 * stability
        
        return {
            'score': score,
            'final_train_loss': final_train_loss,
            'final_val_loss': final_val_loss,
            'convergence_speed': convergence_speed,
            'stability': stability
        }
    
    def find_best_optimizer(self):
        """æ‰¾åˆ°æœ€ä½³ä¼˜åŒ–å™¨é…ç½®"""
        candidates = [
            {
                'name': 'Adam',
                'class': torch.optim.Adam,
                'params': {'lr': 1e-3, 'weight_decay': 1e-4}
            },
            {
                'name': 'AdamW',
                'class': torch.optim.AdamW,
                'params': {'lr': 5e-4, 'weight_decay': 0.01}
            },
            {
                'name': 'SGD',
                'class': torch.optim.SGD,
                'params': {'lr': 1e-2, 'momentum': 0.9, 'weight_decay': 1e-4}
            },
            {
                'name': 'RMSprop',
                'class': torch.optim.RMSprop,
                'params': {'lr': 1e-3, 'alpha': 0.99, 'weight_decay': 1e-4}
            }
        ]
        
        results = {}
        for config in candidates:
            print(f"è¯„ä¼°ä¼˜åŒ–å™¨: {config['name']}")
            result = self.evaluate_optimizer(config)
            result['config'] = config
            results[config['name']] = result
        
        # æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„ä¼˜åŒ–å™¨
        best_optimizer = max(results.items(), key=lambda x: x[1]['score'])
        
        print("\nä¼˜åŒ–å™¨è¯„ä¼°ç»“æœ:")
        for name, result in results.items():
            print(f"{name}: Score={result['score']:.4f}, "
                  f"Val Loss={result['final_val_loss']:.4f}")
        
        print(f"\næ¨èä½¿ç”¨: {best_optimizer[0]}")
        return best_optimizer[1]['config']
```

---

## ğŸ”— ä¸ƒã€æ€»ç»“ä¸æœ€ä½³å®è·µ

### 7.1 å¿«é€Ÿé€‰æ‹©æŒ‡å—

**æ–°æ‰‹æ¨èé¡ºåº**ï¼š
1. **Adam** - ä¸‡èƒ½é€‰æ‹©ï¼Œé€‚åˆå¤§å¤šæ•°æƒ…å†µ
2. **AdamW** - ç°ä»£æ·±åº¦å­¦ä¹ é¦–é€‰
3. **SGD+Momentum** - ç»å…¸å¯é ï¼Œæ³›åŒ–èƒ½åŠ›å¼º

**æ ¹æ®èµ„æºé€‰æ‹©**ï¼š
- **GPUå†…å­˜å……è¶³**: AdamW > Adam
- **GPUå†…å­˜å—é™**: SGD > RMSprop
- **CPUè®­ç»ƒ**: SGD

**æ ¹æ®æ•°æ®é‡é€‰æ‹©**ï¼š
- **å¤§æ•°æ®é›†(>1M)**: SGD+Momentum
- **ä¸­ç­‰æ•°æ®é›†(10K~1M)**: Adam/AdamW
- **å°æ•°æ®é›†(<10K)**: Adam

### 7.2 è°ƒå‚æœ€ä½³å®è·µ

```python
def get_recommended_config(model_size, dataset_size, task_type):
    """è·å–æ¨èçš„ä¼˜åŒ–å™¨é…ç½®"""
    
    if task_type == 'nlp_transformer':
        return {
            'optimizer': 'AdamW',
            'lr': 5e-4,
            'weight_decay': 0.01,
            'scheduler': 'cosine_warmup'
        }
    elif dataset_size == 'large':
        return {
            'optimizer': 'SGD',
            'lr': 0.1,
            'momentum': 0.9,
            'weight_decay': 1e-4,
            'scheduler': 'step'
        }
    else:
        return {
            'optimizer': 'Adam',
            'lr': 1e-3,
            'weight_decay': 1e-4,
            'scheduler': 'plateau'
        }
```

### 7.3 ç›‘æ§ä¸è°ƒè¯•

```python
class OptimizerMonitor:
    def __init__(self, optimizer):
        self.optimizer = optimizer
        self.lr_history = []
        self.grad_norm_history = []
        
    def step(self, model):
        # è®°å½•å­¦ä¹ ç‡
        self.lr_history.append(self.optimizer.param_groups[0]['lr'])
        
        # è®°å½•æ¢¯åº¦èŒƒæ•°
        total_norm = 0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        total_norm = total_norm ** 0.5
        self.grad_norm_history.append(total_norm)
        
        # æ£€æŸ¥å¼‚å¸¸
        if total_norm > 100:
            print(f"è­¦å‘Š: æ¢¯åº¦èŒƒæ•°è¿‡å¤§ {total_norm:.2f}")
        elif total_norm < 1e-6:
            print(f"è­¦å‘Š: æ¢¯åº¦èŒƒæ•°è¿‡å° {total_norm:.2e}")
    
    def plot_metrics(self):
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        ax1.plot(self.lr_history)
        ax1.set_title('Learning Rate')
        ax1.set_xlabel('Step')
        ax1.set_ylabel('LR')
        
        ax2.plot(self.grad_norm_history)
        ax2.set_title('Gradient Norm')
        ax2.set_xlabel('Step')
        ax2.set_ylabel('Norm')
        
        plt.tight_layout()
        plt.show()
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- **é‡å­ä¼˜åŒ–**: [[K1-åŸºç¡€ç†è®ºä¸æ¦‚å¿µ/è®¡ç®—åŸºç¡€/é‡å­è®¡ç®—é¿å…å±€éƒ¨æœ€ä¼˜ï¼šåŸç†ã€æŒ‘æˆ˜ä¸AIåº”ç”¨å‰æ²¿|é‡å­è®¡ç®—é¿å…å±€éƒ¨æœ€ä¼˜ï¼šåŸç†ã€æŒ‘æˆ˜ä¸AIåº”ç”¨å‰æ²¿]]
- **æŸå¤±å‡½æ•°**: [[K2-æŠ€æœ¯æ–¹æ³•ä¸å®ç°/è®­ç»ƒæŠ€æœ¯/Losså‡½æ•°ä¸æ¨¡å‹è°ƒä¼˜å…¨é¢æŒ‡å—|Losså‡½æ•°ä¸æ¨¡å‹è°ƒä¼˜å…¨é¢æŒ‡å—]]
- **æ­£åˆ™åŒ–æŠ€æœ¯**: [[K2-æŠ€æœ¯æ–¹æ³•ä¸å®ç°/ä¼˜åŒ–æ–¹æ³•/æ·±åº¦å­¦ä¹ æ­£åˆ™åŒ–æŠ€æœ¯å…¨é¢æŒ‡å—|æ·±åº¦å­¦ä¹ æ­£åˆ™åŒ–æŠ€æœ¯å…¨é¢æŒ‡å—]]
- **Hugging Faceç”Ÿæ€**: [[K3-å·¥å…·å¹³å°ä¸ç”Ÿæ€/å¼€å‘å¹³å°/Hugging Faceç”Ÿæ€å…¨é¢æŒ‡å—|Hugging Faceç”Ÿæ€å…¨é¢æŒ‡å—]]

---

**æ›´æ–°æ—¶é—´**: 2025å¹´1æœˆ  
**ç»´æŠ¤è€…**: AIçŸ¥è¯†åº“å›¢é˜Ÿ  
**éš¾åº¦è¯„çº§**: â­â­â­â­ (éœ€è¦æ‰å®çš„æ•°å­¦åŸºç¡€å’Œå®è·µç»éªŒ)
