# 深度学习优化器算法对比分析

> **标签**: 优化算法 | 梯度下降 | 自适应学习率 | 模型训练  
> **适用场景**: 模型训练优化、算法选择、超参数调优  
> **难度级别**: ⭐⭐⭐⭐
> **关联**：[[K1-基础理论与概念/核心概念/损失函数与训练调优术语名词库|术语名词库（大白话对照）]]

## 📋 概述

优化器（Optimizer）是深度学习训练的核心引擎，负责根据损失函数的梯度更新模型参数。不同的优化器采用不同的策略来加速收敛、避免局部最优、提高训练稳定性。本文将深入分析主流优化器的原理、特点和适用场景。

---

## 🎯 一、优化器基础理论

### 1.1 梯度下降基本原理

**核心思想**：沿着损失函数梯度的反方向更新参数，逐步找到最优解。

**基本公式**：
```
θ(t+1) = θ(t) - η∇J(θ(t))

其中：
- θ：模型参数
- η：学习率
- ∇J(θ)：损失函数关于参数的梯度
- t：时间步（迭代次数）
```

### 1.2 优化器设计考虑因素

| 考虑因素 | 说明 | 影响 |
|----------|------|------|
| **收敛速度** | 达到最优解的快慢 | 训练时间 |
| **收敛稳定性** | 训练过程是否平滑 | 训练成功率 |
| **泛化能力** | 避免过拟合的能力 | 模型性能 |
| **内存使用** | 额外状态存储需求 | 资源消耗 |
| **超参数敏感性** | 对学习率等参数的依赖 | 调参难度 |

---

## 🔧 二、主流优化器详细分析

### 2.1 SGD (Stochastic Gradient Descent)

#### 基本原理
最基础的梯度下降算法，每次使用一个批次的数据计算梯度。

#### 数学公式
```
θ(t+1) = θ(t) - η∇J(θ(t))
```

#### 带动量的SGD (SGD with Momentum)
```
v(t+1) = βv(t) + η∇J(θ(t))
θ(t+1) = θ(t) - v(t+1)

其中：β为动量系数，通常取0.9
```

#### 代码实现
```python
class SGD:
    def __init__(self, params, lr=0.01, momentum=0.9):
        self.params = params
        self.lr = lr
        self.momentum = momentum
        self.velocity = [torch.zeros_like(p) for p in params]
    
    def step(self, gradients):
        for i, (param, grad) in enumerate(zip(self.params, gradients)):
            self.velocity[i] = self.momentum * self.velocity[i] + self.lr * grad
            param.data -= self.velocity[i]
```

#### 优势与劣势
**✅ 优势**：
- 简单可靠，理论基础扎实
- 内存占用少
- 泛化能力强，不容易过拟合
- 在大数据集上表现良好

**❌ 劣势**：
- 收敛速度相对较慢
- 对学习率敏感，需要精心调参
- 在鞍点处容易陷入振荡
- 处理稀疏梯度效率低

#### 适用场景
- 大规模数据集训练
- 计算资源受限的环境
- 需要良好泛化性能的场景
- 传统CNN、RNN训练

### 2.2 Adam (Adaptive Moment Estimation)

#### 基本原理
结合了动量和自适应学习率，维护每个参数的一阶矩估计（动量）和二阶矩估计（梯度平方的指数移动平均）。

#### 数学公式
```
m(t) = β1*m(t-1) + (1-β1)*g(t)          # 一阶矩估计
v(t) = β2*v(t-1) + (1-β2)*g(t)²         # 二阶矩估计

m̂(t) = m(t) / (1-β1^t)                  # 偏差修正
v̂(t) = v(t) / (1-β2^t)                  # 偏差修正

θ(t+1) = θ(t) - η * m̂(t) / (√v̂(t) + ε)

默认参数：β1=0.9, β2=0.999, ε=1e-8
```

#### 代码实现
```python
class Adam:
    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        self.params = params
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.m = [torch.zeros_like(p) for p in params]
        self.v = [torch.zeros_like(p) for p in params]
        self.step_count = 0
    
    def step(self, gradients):
        self.step_count += 1
        
        for i, (param, grad) in enumerate(zip(self.params, gradients)):
            # 更新动量
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad**2
            
            # 偏差修正
            m_hat = self.m[i] / (1 - self.beta1**self.step_count)
            v_hat = self.v[i] / (1 - self.beta2**self.step_count)
            
            # 参数更新
            param.data -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)
```

#### 优势与劣势
**✅ 优势**：
- 自适应学习率，对不同参数采用不同的更新步长
- 对超参数相对不敏感
- 收敛速度快，特别是在训练初期
- 处理稀疏梯度效果好
- 广泛适用于各种深度学习任务

**❌ 劣势**：
- 内存占用较大（需存储一阶和二阶矩）
- 可能收敛到较差的局部最优
- 在某些情况下泛化性能不如SGD
- 学习率衰减可能导致过早停止学习

#### 适用场景
- 快速原型开发
- 复杂模型（Transformer、GAN等）
- 稀疏数据处理
- 需要快速收敛的场景

### 2.3 AdamW (Adam with Weight Decay)

#### 基本原理
在Adam的基础上修正了权重衰减的实现方式，将权重衰减从梯度中分离出来。

#### 数学公式
```
m(t) = β1*m(t-1) + (1-β1)*g(t)
v(t) = β2*v(t-1) + (1-β2)*g(t)²

m̂(t) = m(t) / (1-β1^t)
v̂(t) = v(t) / (1-β2^t)

θ(t+1) = θ(t) - η * (m̂(t) / (√v̂(t) + ε) + λ*θ(t))

其中λ为权重衰减系数
```

#### 代码实现
```python
class AdamW:
    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01):
        self.params = params
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.weight_decay = weight_decay
        self.m = [torch.zeros_like(p) for p in params]
        self.v = [torch.zeros_like(p) for p in params]
        self.step_count = 0
    
    def step(self, gradients):
        self.step_count += 1
        
        for i, (param, grad) in enumerate(zip(self.params, gradients)):
            # 权重衰减
            param.data.mul_(1 - self.lr * self.weight_decay)
            
            # Adam更新
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad**2
            
            m_hat = self.m[i] / (1 - self.beta1**self.step_count)
            v_hat = self.v[i] / (1 - self.beta2**self.step_count)
            
            param.data -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)
```

#### 优势
- 修正了Adam中权重衰减的问题
- 更好的正则化效果
- 在Transformer等模型上表现优秀
- 平衡了收敛速度和泛化能力

### 2.4 RMSprop (Root Mean Square Propagation)

#### 基本原理
使用梯度平方的指数移动平均来调整学习率，解决Adagrad学习率衰减过快的问题。

#### 数学公式
```
v(t) = β*v(t-1) + (1-β)*g(t)²
θ(t+1) = θ(t) - η*g(t) / (√v(t) + ε)

其中β通常取0.9
```

#### 代码实现
```python
class RMSprop:
    def __init__(self, params, lr=0.01, beta=0.9, eps=1e-8):
        self.params = params
        self.lr = lr
        self.beta = beta
        self.eps = eps
        self.v = [torch.zeros_like(p) for p in params]
    
    def step(self, gradients):
        for i, (param, grad) in enumerate(zip(self.params, gradients)):
            self.v[i] = self.beta * self.v[i] + (1 - self.beta) * grad**2
            param.data -= self.lr * grad / (torch.sqrt(self.v[i]) + self.eps)
```

#### 优势与劣势
**✅ 优势**：
- 自适应学习率调整
- 适合处理非平稳数据
- 在RNN训练中表现良好
- 内存占用适中

**❌ 劣势**：
- 仍可能出现学习率过度衰减
- 对超参数敏感
- 收敛速度不如Adam

### 2.5 Adagrad (Adaptive Gradient)

#### 基本原理
累积历史梯度的平方和来自适应调整学习率，给频繁更新的参数更小的学习率。

#### 数学公式
```
G(t) = G(t-1) + g(t)²                    # 累积梯度平方
θ(t+1) = θ(t) - η*g(t) / (√G(t) + ε)
```

#### 优势与劣势
**✅ 优势**：
- 自动调整学习率
- 对稀疏特征友好
- 理论基础扎实

**❌ 劣势**：
- 学习率单调递减，可能过早停止学习
- 在深度学习中很少使用

---

## 📊 三、优化器性能对比

### 3.1 收敛速度对比

```python
import matplotlib.pyplot as plt
import numpy as np

def compare_optimizers_convergence():
    """模拟不同优化器的收敛曲线"""
    epochs = 100
    
    # 模拟损失曲线
    sgd_loss = 2.0 * np.exp(-0.05 * np.arange(epochs)) + 0.1 + 0.05 * np.random.randn(epochs)
    adam_loss = 2.0 * np.exp(-0.12 * np.arange(epochs)) + 0.15 + 0.03 * np.random.randn(epochs)
    adamw_loss = 2.0 * np.exp(-0.10 * np.arange(epochs)) + 0.12 + 0.02 * np.random.randn(epochs)
    rmsprop_loss = 2.0 * np.exp(-0.08 * np.arange(epochs)) + 0.13 + 0.04 * np.random.randn(epochs)
    
    plt.figure(figsize=(12, 8))
    plt.plot(sgd_loss, label='SGD', linewidth=2)
    plt.plot(adam_loss, label='Adam', linewidth=2)
    plt.plot(adamw_loss, label='AdamW', linewidth=2)
    plt.plot(rmsprop_loss, label='RMSprop', linewidth=2)
    
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Optimizers Convergence Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
```

### 3.2 详细性能对比表

| 优化器 | 收敛速度 | 内存使用 | 超参数敏感性 | 泛化能力 | 稳定性 | 适用场景 |
|--------|----------|----------|-------------|----------|--------|----------|
| **SGD** | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | 大数据集、CNN |
| **SGD+Momentum** | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 传统深度学习 |
| **Adam** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | 快速原型、复杂模型 |
| **AdamW** | ⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | Transformer、现代DL |
| **RMSprop** | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | RNN、非平稳数据 |
| **Adagrad** | ⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ | ⭐⭐ | 稀疏数据 |

### 3.3 不同任务类型的推荐

```python
def get_optimizer_recommendation(task_type, dataset_size, compute_budget):
    """根据任务特点推荐优化器"""
    
    recommendations = {
        'image_classification': {
            'large_dataset': 'SGD with Momentum',
            'small_dataset': 'Adam',
            'limited_compute': 'SGD'
        },
        'nlp_transformer': {
            'any_size': 'AdamW',
            'limited_compute': 'Adam'
        },
        'rnn_sequence': {
            'any_size': 'RMSprop',
            'alternative': 'Adam'
        },
        'gan_training': {
            'discriminator': 'Adam',
            'generator': 'Adam'
        },
        'reinforcement_learning': {
            'policy_gradient': 'Adam',
            'value_function': 'RMSprop'
        }
    }
    
    if task_type in recommendations:
        if dataset_size == 'large':
            return recommendations[task_type].get('large_dataset', 'SGD')
        elif compute_budget == 'limited':
            return recommendations[task_type].get('limited_compute', 'Adam')
        else:
            return recommendations[task_type].get('any_size', 'Adam')
    else:
        return 'Adam (通用选择)'
```

---

## 🛠️ 四、优化器选择策略

### 4.1 决策树指南

```
开始训练新模型
├── 是否为Transformer架构？
│   ├── 是 → 推荐AdamW (lr=1e-3~5e-4)
│   └── 否 → 继续下一步
├── 数据集规模如何？
│   ├── 大规模(>1M样本) → 推荐SGD+Momentum (lr=1e-1~1e-2)
│   ├── 中规模(10K~1M) → 推荐Adam (lr=1e-3~1e-4)
│   └── 小规模(<10K) → 推荐Adam (lr=1e-3~1e-4)
├── 计算资源是否受限？
│   ├── 是 → 推荐SGD
│   └── 否 → 根据任务类型选择
└── 是否需要快速收敛？
    ├── 是 → 推荐Adam
    └── 否 → 推荐SGD+Momentum
```

### 4.2 超参数设置指南

#### SGD推荐设置
```python
# 图像分类
optimizer = torch.optim.SGD(
    model.parameters(), 
    lr=0.1,                    # 初始学习率
    momentum=0.9,              # 动量系数
    weight_decay=1e-4,         # 权重衰减
    nesterov=True              # Nesterov动量
)

# 学习率调度
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
```

#### Adam推荐设置
```python
# 通用设置
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=1e-3,                   # 学习率
    betas=(0.9, 0.999),        # 动量参数
    eps=1e-8,                  # 数值稳定性
    weight_decay=1e-4          # 权重衰减
)
```

#### AdamW推荐设置
```python
# Transformer模型
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=5e-4,                   # 较小的学习率
    betas=(0.9, 0.999),        # 标准动量参数
    eps=1e-8,
    weight_decay=0.01          # 较强的权重衰减
)

# 配合余弦退火调度器
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
```

### 4.3 动态优化器切换

```python
class AdaptiveOptimizerSwitcher:
    def __init__(self, model, initial_optimizer='adam'):
        self.model = model
        self.current_optimizer = initial_optimizer
        self.loss_history = []
        self.switch_patience = 20
        self.no_improvement_count = 0
        
        # 初始化优化器
        self.optimizers = {
            'adam': torch.optim.Adam(model.parameters(), lr=1e-3),
            'sgd': torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9),
            'adamw': torch.optim.AdamW(model.parameters(), lr=5e-4)
        }
    
    def step(self, loss):
        """根据loss变化决定是否切换优化器"""
        self.loss_history.append(loss)
        
        # 检查是否需要切换
        if len(self.loss_history) >= self.switch_patience:
            recent_improvement = self.loss_history[-self.switch_patience] - loss
            
            if recent_improvement < 0.001:  # 几乎没有改善
                self.no_improvement_count += 1
                
                if self.no_improvement_count >= 5:
                    self.switch_optimizer()
                    self.no_improvement_count = 0
            else:
                self.no_improvement_count = 0
    
    def switch_optimizer(self):
        """切换到下一个优化器"""
        switch_order = ['adam', 'adamw', 'sgd']
        current_idx = switch_order.index(self.current_optimizer)
        next_idx = (current_idx + 1) % len(switch_order)
        
        self.current_optimizer = switch_order[next_idx]
        print(f"切换到优化器: {self.current_optimizer}")
    
    def get_current_optimizer(self):
        return self.optimizers[self.current_optimizer]
```

---

## 📈 五、高级优化技术

### 5.1 学习率调度策略

#### 步进衰减 (Step Decay)
```python
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
```

#### 余弦退火 (Cosine Annealing)
```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
```

#### 余弦重启 (Cosine Annealing with Warm Restarts)
```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer, T_0=10, T_mult=2
)
```

#### 自定义Warmup + 余弦衰减
```python
class WarmupCosineScheduler:
    def __init__(self, optimizer, warmup_epochs, total_epochs, base_lr, min_lr=1e-6):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs
        self.base_lr = base_lr
        self.min_lr = min_lr
    
    def step(self, epoch):
        if epoch < self.warmup_epochs:
            # Warmup阶段
            lr = self.base_lr * epoch / self.warmup_epochs
        else:
            # 余弦衰减阶段
            progress = (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)
            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + math.cos(math.pi * progress))
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
```

### 5.2 梯度处理技术

#### 梯度裁剪
```python
# 按范数裁剪
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 按值裁剪
torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)
```

#### 梯度累积
```python
def train_with_gradient_accumulation(model, dataloader, optimizer, accumulation_steps=4):
    model.train()
    optimizer.zero_grad()
    
    for i, (inputs, targets) in enumerate(dataloader):
        outputs = model(inputs)
        loss = criterion(outputs, targets) / accumulation_steps
        loss.backward()
        
        if (i + 1) % accumulation_steps == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            optimizer.zero_grad()
```

### 5.3 二阶优化方法

#### L-BFGS (Limited-memory BFGS)
```python
# 适用于小数据集的二阶优化
optimizer = torch.optim.LBFGS(model.parameters(), lr=1.0)

def closure():
    optimizer.zero_grad()
    output = model(input)
    loss = criterion(output, target)
    loss.backward()
    return loss

optimizer.step(closure)
```

---

## 🎯 六、实战优化策略

### 6.1 不同模型架构的优化器选择

```python
class ModelSpecificOptimizer:
    @staticmethod
    def get_optimizer(model_type, model, **kwargs):
        """根据模型类型返回最适合的优化器配置"""
        
        configs = {
            'resnet': {
                'optimizer': torch.optim.SGD,
                'params': {
                    'lr': 0.1,
                    'momentum': 0.9,
                    'weight_decay': 1e-4,
                    'nesterov': True
                }
            },
            'transformer': {
                'optimizer': torch.optim.AdamW,
                'params': {
                    'lr': 5e-4,
                    'betas': (0.9, 0.999),
                    'weight_decay': 0.01,
                    'eps': 1e-8
                }
            },
            'lstm': {
                'optimizer': torch.optim.RMSprop,
                'params': {
                    'lr': 1e-3,
                    'alpha': 0.99,
                    'eps': 1e-8,
                    'weight_decay': 1e-4
                }
            },
            'gan_generator': {
                'optimizer': torch.optim.Adam,
                'params': {
                    'lr': 2e-4,
                    'betas': (0.5, 0.999)
                }
            },
            'gan_discriminator': {
                'optimizer': torch.optim.Adam,
                'params': {
                    'lr': 2e-4,
                    'betas': (0.5, 0.999)
                }
            }
        }
        
        if model_type in configs:
            config = configs[model_type]
            # 合并用户提供的参数
            params = {**config['params'], **kwargs}
            return config['optimizer'](model.parameters(), **params)
        else:
            # 默认选择Adam
            return torch.optim.Adam(model.parameters(), lr=1e-3)
```

### 6.2 自适应优化器选择系统

```python
class SmartOptimizerSelector:
    def __init__(self, model, train_loader, val_loader, trial_epochs=5):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.trial_epochs = trial_epochs
        
    def evaluate_optimizer(self, optimizer_config):
        """评估单个优化器配置的性能"""
        model_copy = copy.deepcopy(self.model)
        optimizer = optimizer_config['class'](
            model_copy.parameters(), 
            **optimizer_config['params']
        )
        
        train_losses = []
        val_losses = []
        
        for epoch in range(self.trial_epochs):
            # 训练
            model_copy.train()
            train_loss = 0
            for batch_idx, (data, target) in enumerate(self.train_loader):
                optimizer.zero_grad()
                output = model_copy(data)
                loss = F.cross_entropy(output, target)
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
                
                if batch_idx > 10:  # 只用少量批次评估
                    break
            
            train_losses.append(train_loss / min(len(self.train_loader), 11))
            
            # 验证
            model_copy.eval()
            val_loss = 0
            with torch.no_grad():
                for batch_idx, (data, target) in enumerate(self.val_loader):
                    output = model_copy(data)
                    val_loss += F.cross_entropy(output, target).item()
                    if batch_idx > 5:  # 少量验证
                        break
            
            val_losses.append(val_loss / min(len(self.val_loader), 6))
        
        # 评估指标：最终损失 + 收敛速度 + 稳定性
        final_train_loss = train_losses[-1]
        final_val_loss = val_losses[-1]
        convergence_speed = train_losses[0] - train_losses[-1]
        stability = -np.std(val_losses)  # 负标准差，越大越稳定
        
        score = -final_val_loss + 0.3 * convergence_speed + 0.1 * stability
        
        return {
            'score': score,
            'final_train_loss': final_train_loss,
            'final_val_loss': final_val_loss,
            'convergence_speed': convergence_speed,
            'stability': stability
        }
    
    def find_best_optimizer(self):
        """找到最佳优化器配置"""
        candidates = [
            {
                'name': 'Adam',
                'class': torch.optim.Adam,
                'params': {'lr': 1e-3, 'weight_decay': 1e-4}
            },
            {
                'name': 'AdamW',
                'class': torch.optim.AdamW,
                'params': {'lr': 5e-4, 'weight_decay': 0.01}
            },
            {
                'name': 'SGD',
                'class': torch.optim.SGD,
                'params': {'lr': 1e-2, 'momentum': 0.9, 'weight_decay': 1e-4}
            },
            {
                'name': 'RMSprop',
                'class': torch.optim.RMSprop,
                'params': {'lr': 1e-3, 'alpha': 0.99, 'weight_decay': 1e-4}
            }
        ]
        
        results = {}
        for config in candidates:
            print(f"评估优化器: {config['name']}")
            result = self.evaluate_optimizer(config)
            result['config'] = config
            results[config['name']] = result
        
        # 找到得分最高的优化器
        best_optimizer = max(results.items(), key=lambda x: x[1]['score'])
        
        print("\n优化器评估结果:")
        for name, result in results.items():
            print(f"{name}: Score={result['score']:.4f}, "
                  f"Val Loss={result['final_val_loss']:.4f}")
        
        print(f"\n推荐使用: {best_optimizer[0]}")
        return best_optimizer[1]['config']
```

---

## 🔗 七、总结与最佳实践

### 7.1 快速选择指南

**新手推荐顺序**：
1. **Adam** - 万能选择，适合大多数情况
2. **AdamW** - 现代深度学习首选
3. **SGD+Momentum** - 经典可靠，泛化能力强

**根据资源选择**：
- **GPU内存充足**: AdamW > Adam
- **GPU内存受限**: SGD > RMSprop
- **CPU训练**: SGD

**根据数据量选择**：
- **大数据集(>1M)**: SGD+Momentum
- **中等数据集(10K~1M)**: Adam/AdamW
- **小数据集(<10K)**: Adam

### 7.2 调参最佳实践

```python
def get_recommended_config(model_size, dataset_size, task_type):
    """获取推荐的优化器配置"""
    
    if task_type == 'nlp_transformer':
        return {
            'optimizer': 'AdamW',
            'lr': 5e-4,
            'weight_decay': 0.01,
            'scheduler': 'cosine_warmup'
        }
    elif dataset_size == 'large':
        return {
            'optimizer': 'SGD',
            'lr': 0.1,
            'momentum': 0.9,
            'weight_decay': 1e-4,
            'scheduler': 'step'
        }
    else:
        return {
            'optimizer': 'Adam',
            'lr': 1e-3,
            'weight_decay': 1e-4,
            'scheduler': 'plateau'
        }
```

### 7.3 监控与调试

```python
class OptimizerMonitor:
    def __init__(self, optimizer):
        self.optimizer = optimizer
        self.lr_history = []
        self.grad_norm_history = []
        
    def step(self, model):
        # 记录学习率
        self.lr_history.append(self.optimizer.param_groups[0]['lr'])
        
        # 记录梯度范数
        total_norm = 0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        total_norm = total_norm ** 0.5
        self.grad_norm_history.append(total_norm)
        
        # 检查异常
        if total_norm > 100:
            print(f"警告: 梯度范数过大 {total_norm:.2f}")
        elif total_norm < 1e-6:
            print(f"警告: 梯度范数过小 {total_norm:.2e}")
    
    def plot_metrics(self):
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        ax1.plot(self.lr_history)
        ax1.set_title('Learning Rate')
        ax1.set_xlabel('Step')
        ax1.set_ylabel('LR')
        
        ax2.plot(self.grad_norm_history)
        ax2.set_title('Gradient Norm')
        ax2.set_xlabel('Step')
        ax2.set_ylabel('Norm')
        
        plt.tight_layout()
        plt.show()
```

## 🔗 相关文档

- **量子优化**: [[K1-基础理论与概念/计算基础/量子计算避免局部最优：原理、挑战与AI应用前沿|量子计算避免局部最优：原理、挑战与AI应用前沿]]
- **损失函数**: [[K2-技术方法与实现/训练技术/Loss函数与模型调优全面指南|Loss函数与模型调优全面指南]]
- **正则化技术**: [[K2-技术方法与实现/优化方法/深度学习正则化技术全面指南|深度学习正则化技术全面指南]]
- **Hugging Face生态**: [[K3-工具平台与生态/开发平台/Hugging Face生态全面指南|Hugging Face生态全面指南]]

---

**更新时间**: 2025年1月  
**维护者**: AI知识库团队  
**难度评级**: ⭐⭐⭐⭐ (需要扎实的数学基础和实践经验)
