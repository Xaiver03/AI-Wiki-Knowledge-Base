
> **æ ‡ç­¾**: æŸå¤±å‡½æ•° | æ·±åº¦å­¦ä¹  | æ¨¡å‹è®­ç»ƒ | ç®—æ³•é€‰æ‹©  
> **é€‚ç”¨åœºæ™¯**: æ¨¡å‹è®¾è®¡ã€ä»»åŠ¡é€‚é…ã€æ€§èƒ½ä¼˜åŒ–  
> **éš¾åº¦çº§åˆ«**: â­â­â­â­

## ğŸ“‹ æ¦‚è¿°

æŸå¤±å‡½æ•°æ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒç»„ä»¶ï¼Œä¸åŒçš„ä»»åŠ¡ç±»å‹éœ€è¦ä¸åŒçš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚é€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°ç›´æ¥å½±å“æ¨¡å‹çš„è®­ç»ƒæ•ˆæœå’Œæœ€ç»ˆæ€§èƒ½ã€‚æœ¬æ–‡å°†è¯¦ç»†ä»‹ç»å„ç±»æŸå¤±å‡½æ•°çš„åŸç†ã€ç‰¹ç‚¹ã€å®ç°å’Œåº”ç”¨åœºæ™¯ã€‚

## ğŸ§  æœ¯è¯­é€ŸæŸ¥è¡¨ï¼ˆå¤§ç™½è¯ vs ä¸“ä¸šè§£é‡Šï¼‰

å‚è§ç»Ÿä¸€åè¯åº“ï¼š[[K1-åŸºç¡€ç†è®ºä¸æ¦‚å¿µ/æ ¸å¿ƒæ¦‚å¿µ/æŸå¤±å‡½æ•°ä¸è®­ç»ƒè°ƒä¼˜æœ¯è¯­åè¯åº“|æœ¯è¯­åè¯åº“ï¼ˆå¤§ç™½è¯å¯¹ç…§ï¼‰]]

## ğŸ”— ç›¸å…³æ–‡æ¡£é“¾æ¥

- **å‘é‡æŠ€æœ¯**: [[K2-æŠ€æœ¯æ–¹æ³•ä¸å®ç°/å‘é‡æŠ€æœ¯/Embeddingå‘é‡åµŒå…¥æŠ€æœ¯å…¨é¢æ•™ç¨‹|Embeddingå‘é‡åµŒå…¥æŠ€æœ¯å…¨é¢æ•™ç¨‹]]
- **è®­ç»ƒè°ƒä¼˜**: [[K2-æŠ€æœ¯æ–¹æ³•ä¸å®ç°/è®­ç»ƒæŠ€æœ¯/Losså‡½æ•°ä¸æ¨¡å‹è°ƒä¼˜å…¨é¢æŒ‡å—|Losså‡½æ•°ä¸æ¨¡å‹è°ƒä¼˜å…¨é¢æŒ‡å—]]
- **ä¼˜åŒ–å™¨ç®—æ³•**: [[K2-æŠ€æœ¯æ–¹æ³•ä¸å®ç°/ä¼˜åŒ–æ–¹æ³•/æ·±åº¦å­¦ä¹ ä¼˜åŒ–å™¨ç®—æ³•å¯¹æ¯”åˆ†æ|æ·±åº¦å­¦ä¹ ä¼˜åŒ–å™¨ç®—æ³•å¯¹æ¯”åˆ†æ]]
- **æ­£åˆ™åŒ–æŠ€æœ¯**: [[K2-æŠ€æœ¯æ–¹æ³•ä¸å®ç°/ä¼˜åŒ–æ–¹æ³•/æ·±åº¦å­¦ä¹ æ­£åˆ™åŒ–æŠ€æœ¯å…¨é¢æŒ‡å—|æ·±åº¦å­¦ä¹ æ­£åˆ™åŒ–æŠ€æœ¯å…¨é¢æŒ‡å—]]

---

## ğŸ¯ ä¸€ã€æŸå¤±å‡½æ•°åˆ†ç±»ä½“ç³»

### 1.1 æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç±»

```
æŸå¤±å‡½æ•°åˆ†ç±»
â”œâ”€â”€ å›å½’ä»»åŠ¡
â”‚   â”œâ”€â”€ L1 Loss (MAE)
â”‚   â”œâ”€â”€ L2 Loss (MSE)
â”‚   â”œâ”€â”€ Huber Loss
â”‚   â”œâ”€â”€ Log-Cosh Loss
â”‚   â””â”€â”€ Quantile Loss
â”œâ”€â”€ åˆ†ç±»ä»»åŠ¡
â”‚   â”œâ”€â”€ äºŒåˆ†ç±»
â”‚   â”‚   â”œâ”€â”€ Binary Cross-Entropy
â”‚   â”‚   â”œâ”€â”€ Hinge Loss
â”‚   â”‚   â””â”€â”€ Focal Loss
â”‚   â”œâ”€â”€ å¤šåˆ†ç±»
â”‚   â”‚   â”œâ”€â”€ Categorical Cross-Entropy
â”‚   â”‚   â”œâ”€â”€ Sparse Cross-Entropy
â”‚   â”‚   â””â”€â”€ Label Smoothing
â”‚   â””â”€â”€ å¤šæ ‡ç­¾
â”‚       â””â”€â”€ Binary Cross-Entropy per Label
â”œâ”€â”€ æ’åºä»»åŠ¡
â”‚   â”œâ”€â”€ Ranking Loss
â”‚   â”œâ”€â”€ Triplet Loss
â”‚   â””â”€â”€ Contrastive Loss
â”œâ”€â”€ ç”Ÿæˆä»»åŠ¡
â”‚   â”œâ”€â”€ Adversarial Loss
â”‚   â”œâ”€â”€ Reconstruction Loss
â”‚   â””â”€â”€ Perceptual Loss
â””â”€â”€ ç‰¹æ®Šä»»åŠ¡
    â”œâ”€â”€ Dice Loss (åˆ†å‰²)
    â”œâ”€â”€ IoU Loss (æ£€æµ‹)
    â””â”€â”€ CTC Loss (åºåˆ—)
```

---

## ğŸ“Š äºŒã€å›å½’ä»»åŠ¡æŸå¤±å‡½æ•°

### 2.1 L1 Loss (Mean Absolute Error, MAE)

#### æ•°å­¦å®šä¹‰
```
L1(y, Å·) = (1/n) Ã— Î£|yi - Å·i|
```

#### ç‰¹ç‚¹åˆ†æ
- **é²æ£’æ€§å¼º**: å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ
- **ç¨€ç–æ€§**: äº§ç”Ÿç¨€ç–è§£
- **ä¸å¯å¾®**: åœ¨0å¤„ä¸å¯å¾®åˆ†

#### ä»£ç å®ç°
```python
import torch
import torch.nn as nn

class L1Loss(nn.Module):
    def __init__(self, reduction='mean'):
        super().__init__()
        self.reduction = reduction
    
    def forward(self, pred, target):
        loss = torch.abs(pred - target)
        
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss

# ä½¿ç”¨ç¤ºä¾‹
l1_loss = nn.L1Loss()
pred = torch.randn(10, 1)
target = torch.randn(10, 1)
loss = l1_loss(pred, target)
```

#### é€‚ç”¨åœºæ™¯
- å­˜åœ¨å¼‚å¸¸å€¼çš„å›å½’é—®é¢˜
- éœ€è¦ç¨€ç–è§£çš„åœºæ™¯
- å¯¹é¢„æµ‹è¯¯å·®çš„ç»å¯¹å€¼å…³å¿ƒæ›´å¤š

### 2.2 L2 Loss (Mean Squared Error, MSE)

#### æ•°å­¦å®šä¹‰
```
L2(y, Å·) = (1/n) Ã— Î£(yi - Å·i)Â²
```

#### ç‰¹ç‚¹åˆ†æ
- **å¹³æ»‘å¯å¾®**: å¤„å¤„å¯å¾®åˆ†ï¼Œä¾¿äºä¼˜åŒ–
- **æƒ©ç½šå¤§è¯¯å·®**: å¯¹å¤§è¯¯å·®çš„æƒ©ç½šå‘ˆå¹³æ–¹å¢é•¿
- **å™ªå£°æ•æ„Ÿ**: å®¹æ˜“å—å¼‚å¸¸å€¼å½±å“

#### ä»£ç å®ç°
```python
class MSELoss(nn.Module):
    def __init__(self, reduction='mean'):
        super().__init__()
        self.reduction = reduction
    
    def forward(self, pred, target):
        loss = (pred - target) ** 2
        
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss

# ä¸Root Mean Squared Error (RMSE)çš„å…³ç³»
def rmse_loss(pred, target):
    mse = nn.MSELoss()(pred, target)
    return torch.sqrt(mse)
```

#### é€‚ç”¨åœºæ™¯
- æ ‡å‡†å›å½’é—®é¢˜
- æ•°æ®è´¨é‡è¾ƒå¥½ï¼Œå¼‚å¸¸å€¼è¾ƒå°‘
- éœ€è¦å¹³æ»‘æ¢¯åº¦çš„ä¼˜åŒ–åœºæ™¯

### 2.3 Huber Loss (Smooth L1 Loss)

#### æ•°å­¦å®šä¹‰
```
Huber(y, Å·) = {
    0.5 Ã— (y - Å·)Â²           if |y - Å·| â‰¤ Î´
    Î´ Ã— |y - Å·| - 0.5 Ã— Î´Â²   otherwise
}
```

#### ç‰¹ç‚¹åˆ†æ
- **ç»“åˆä¼˜åŠ¿**: èåˆäº†L1å’ŒL2çš„ä¼˜ç‚¹
- **è‡ªé€‚åº”**: å°è¯¯å·®æ—¶è¡¨ç°å¦‚MSEï¼Œå¤§è¯¯å·®æ—¶è¡¨ç°å¦‚MAE
- **å¯è°ƒèŠ‚**: é€šè¿‡Î´å‚æ•°æ§åˆ¶è½¬æ¢ç‚¹

#### ä»£ç å®ç°
```python
class HuberLoss(nn.Module):
    def __init__(self, delta=1.0, reduction='mean'):
        super().__init__()
        self.delta = delta
        self.reduction = reduction
    
    def forward(self, pred, target):
        error = torch.abs(pred - target)
        
        # å°è¯¯å·®åŒºåŸŸä½¿ç”¨L2
        quadratic = torch.where(error <= self.delta, 
                               0.5 * error**2, 
                               self.delta * error - 0.5 * self.delta**2)
        
        if self.reduction == 'mean':
            return quadratic.mean()
        elif self.reduction == 'sum':
            return quadratic.sum()
        else:
            return quadratic

# å‚æ•°é€‰æ‹©æŒ‡å—
def choose_delta(data_distribution):
    """æ ¹æ®æ•°æ®åˆ†å¸ƒé€‰æ‹©åˆé€‚çš„deltaå€¼"""
    std = torch.std(data_distribution)
    return std * 1.345  # ç»éªŒå€¼ï¼Œçº¦ç­‰äº1.345Ïƒ
```

#### é€‚ç”¨åœºæ™¯
- åŒæ—¶å­˜åœ¨æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸å€¼
- éœ€è¦åœ¨ç¨³å®šæ€§å’Œæ•æ„Ÿæ€§ä¹‹é—´å¹³è¡¡
- å¼ºåŒ–å­¦ä¹ ä¸­çš„ä»·å€¼å‡½æ•°å›å½’

### 2.4 Log-Cosh Loss

#### æ•°å­¦å®šä¹‰
```
Log-Cosh(y, Å·) = Î£ log(cosh(yi - Å·i))
```

#### ç‰¹ç‚¹åˆ†æ
- **è¿‘ä¼¼Huber**: å¹³æ»‘ç‰ˆæœ¬çš„Huber Loss
- **äºŒé˜¶å¯å¾®**: å¤„å¤„äºŒé˜¶å¯å¾®
- **æ¸è¿‘æ€§è´¨**: å°è¯¯å·®æ—¶è¿‘ä¼¼L2ï¼Œå¤§è¯¯å·®æ—¶è¿‘ä¼¼L1

#### ä»£ç å®ç°
```python
class LogCoshLoss(nn.Module):
    def __init__(self, reduction='mean'):
        super().__init__()
        self.reduction = reduction
    
    def forward(self, pred, target):
        error = pred - target
        loss = torch.log(torch.cosh(error))
        
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss

# æ•°å€¼ç¨³å®šç‰ˆæœ¬
class StableLogCoshLoss(nn.Module):
    def forward(self, pred, target):
        error = pred - target
        # ä½¿ç”¨æ•°å€¼ç¨³å®šçš„å®ç°
        # log(cosh(x)) â‰ˆ |x| - log(2) for large |x|
        return torch.mean(torch.where(torch.abs(error) < 12,
                                     torch.log(torch.cosh(error)),
                                     torch.abs(error) - 0.693147))
```

### 2.5 Quantile Loss (åˆ†ä½æ•°æŸå¤±)

#### æ•°å­¦å®šä¹‰
```
Quantile_Ï„(y, Å·) = Î£ max(Ï„(yi - Å·i), (Ï„-1)(yi - Å·i))

å…¶ä¸­Ï„ âˆˆ (0,1)æ˜¯ç›®æ ‡åˆ†ä½æ•°
```

#### ç‰¹ç‚¹åˆ†æ
- **åˆ†ä½æ•°å›å½’**: é¢„æµ‹ç‰¹å®šåˆ†ä½æ•°è€Œéå‡å€¼
- **ä¸å¯¹ç§°**: å¯¹æ­£è´Ÿè¯¯å·®çš„æƒ©ç½šä¸åŒ
- **ç½®ä¿¡åŒºé—´**: å¯ä»¥ä¼°è®¡é¢„æµ‹çš„ä¸ç¡®å®šæ€§

#### ä»£ç å®ç°
```python
class QuantileLoss(nn.Module):
    def __init__(self, quantile=0.5, reduction='mean'):
        super().__init__()
        self.quantile = quantile
        self.reduction = reduction
    
    def forward(self, pred, target):
        error = target - pred
        loss = torch.where(error >= 0,
                          self.quantile * error,
                          (self.quantile - 1) * error)
        
        if self.reduction == 'mean':
            return loss.mean()
        else:
            return loss.sum()

# å¤šåˆ†ä½æ•°é¢„æµ‹
class MultiQuantileLoss(nn.Module):
    def __init__(self, quantiles=[0.1, 0.5, 0.9]):
        super().__init__()
        self.quantiles = quantiles
    
    def forward(self, preds, target):
        """
        preds: [batch_size, num_quantiles]
        target: [batch_size, 1]
        """
        total_loss = 0
        for i, q in enumerate(self.quantiles):
            pred_q = preds[:, i:i+1]
            error = target - pred_q
            loss = torch.where(error >= 0,
                             q * error,
                             (q - 1) * error)
            total_loss += loss.mean()
        
        return total_loss / len(self.quantiles)
```

#### é€‚ç”¨åœºæ™¯
- éœ€è¦é¢„æµ‹ä¸ç¡®å®šæ€§
- é£é™©æ•æ„Ÿçš„å›å½’ä»»åŠ¡
- æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„ç½®ä¿¡åŒºé—´ä¼°è®¡

---

## ğŸ¯ ä¸‰ã€åˆ†ç±»ä»»åŠ¡æŸå¤±å‡½æ•°

### 3.1 Binary Cross-Entropy Loss

#### æ•°å­¦å®šä¹‰
```
BCE(y, Å·) = -(1/n) Ã— Î£[yiÂ·log(Å·i) + (1-yi)Â·log(1-Å·i)]
```

#### ç‰¹ç‚¹åˆ†æ
- **æ¦‚ç‡è§£é‡Š**: åŸºäºæå¤§ä¼¼ç„¶ä¼°è®¡
- **æ¢¯åº¦æ€§è´¨**: åœ¨é¢„æµ‹é”™è¯¯æ—¶æ¢¯åº¦è¾ƒå¤§
- **æ•°å€¼ç¨³å®š**: éœ€è¦æ³¨æ„log(0)çš„æ•°å€¼é—®é¢˜

#### ä»£ç å®ç°
```python
class BinaryCrossEntropyLoss(nn.Module):
    def __init__(self, reduction='mean'):
        super().__init__()
        self.reduction = reduction
    
    def forward(self, pred, target):
        # æ•°å€¼ç¨³å®šç‰ˆæœ¬
        pred = torch.clamp(pred, min=1e-7, max=1-1e-7)
        
        loss = -(target * torch.log(pred) + 
                (1 - target) * torch.log(1 - pred))
        
        if self.reduction == 'mean':
            return loss.mean()
        else:
            return loss.sum()

# ä¸logitsä¸€èµ·ä½¿ç”¨çš„ç¨³å®šç‰ˆæœ¬
class BCEWithLogitsLoss(nn.Module):
    def __init__(self, pos_weight=None, reduction='mean'):
        super().__init__()
        self.pos_weight = pos_weight
        self.reduction = reduction
    
    def forward(self, logits, target):
        # ä½¿ç”¨log-sum-expæŠ€å·§æé«˜æ•°å€¼ç¨³å®šæ€§
        max_val = torch.clamp(-logits, min=0)
        
        if self.pos_weight is not None:
            log_weight = ((self.pos_weight - 1) * target + 1)
            loss = (1 - target) * logits + log_weight * \
                   (torch.log(torch.exp(-max_val) + torch.exp(-logits - max_val)) + max_val)
        else:
            loss = (1 - target) * logits + max_val + \
                   torch.log(torch.exp(-max_val) + torch.exp(-logits - max_val))
        
        if self.reduction == 'mean':
            return loss.mean()
        else:
            return loss.sum()
```

### 3.2 Focal Loss

#### æ•°å­¦å®šä¹‰
```
Focal(y, Å·) = -Î±(1-pt)^Î³ log(pt)

å…¶ä¸­ï¼š
pt = Å· if y = 1 else 1-Å·
Î±: å¹³è¡¡å› å­
Î³: èšç„¦å‚æ•°
```

#### ç‰¹ç‚¹åˆ†æ
- **è§£å†³ä¸å¹³è¡¡**: ä¸“é—¨é’ˆå¯¹ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
- **èšç„¦å›°éš¾æ ·æœ¬**: é™ä½æ˜“åˆ†ç±»æ ·æœ¬çš„æƒé‡
- **å‚æ•°æ•æ„Ÿ**: Î±å’ŒÎ³éœ€è¦ä»”ç»†è°ƒèŠ‚

#### ä»£ç å®ç°
```python
class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, pred, target):
        # äºŒåˆ†ç±»ç‰ˆæœ¬
        pred = torch.clamp(pred, min=1e-7, max=1-1e-7)
        
        # è®¡ç®—äº¤å‰ç†µ
        ce_loss = -(target * torch.log(pred) + 
                   (1 - target) * torch.log(1 - pred))
        
        # è®¡ç®—pt
        pt = torch.where(target == 1, pred, 1 - pred)
        
        # åº”ç”¨focalæƒé‡
        focal_weight = self.alpha * (1 - pt) ** self.gamma
        focal_loss = focal_weight * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        else:
            return focal_loss.sum()

# å¤šåˆ†ç±»Focal Loss
class MultiClassFocalLoss(nn.Module):
    def __init__(self, alpha=None, gamma=2, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, pred, target):
        ce_loss = F.cross_entropy(pred, target, reduction='none')
        pt = torch.exp(-ce_loss)
        
        if self.alpha is not None:
            if isinstance(self.alpha, (float, int)):
                alpha_t = self.alpha
            else:
                alpha_t = self.alpha[target]
        else:
            alpha_t = 1.0
        
        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        else:
            return focal_loss.sum()
```

#### å‚æ•°è°ƒä¼˜æŒ‡å—
```python
def focal_loss_hyperparameter_search(model, train_loader, val_loader):
    """Focal Lossè¶…å‚æ•°æœç´¢"""
    
    alpha_candidates = [0.25, 0.5, 0.75, 1.0]
    gamma_candidates = [0.5, 1.0, 1.5, 2.0, 2.5]
    
    best_score = 0
    best_params = {}
    
    for alpha in alpha_candidates:
        for gamma in gamma_candidates:
            criterion = FocalLoss(alpha=alpha, gamma=gamma)
            
            # å¿«é€Ÿè®­ç»ƒå‡ ä¸ªepochè¯„ä¼°æ•ˆæœ
            score = quick_evaluation(model, train_loader, val_loader, criterion)
            
            if score > best_score:
                best_score = score
                best_params = {'alpha': alpha, 'gamma': gamma}
    
    return best_params
```

### 3.3 Label Smoothing Loss

#### æ•°å­¦å®šä¹‰
```
LabelSmoothing(y, Å·) = (1-Îµ)Â·CE(y, Å·) + ÎµÂ·CE(U, Å·)

å…¶ä¸­ï¼š
Îµ: å¹³æ»‘å‚æ•°
U: å‡åŒ€åˆ†å¸ƒ
CE: äº¤å‰ç†µ
```

#### ç‰¹ç‚¹åˆ†æ
- **æ­£åˆ™åŒ–æ•ˆæœ**: é˜²æ­¢æ¨¡å‹è¿‡åº¦è‡ªä¿¡
- **æ³›åŒ–æå‡**: æ”¹å–„æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
- **æ ‡ç­¾å™ªå£°**: å¯¹æ ‡ç­¾å™ªå£°æœ‰ä¸€å®šé²æ£’æ€§

#### ä»£ç å®ç°
```python
class LabelSmoothingLoss(nn.Module):
    def __init__(self, num_classes, smoothing=0.1, reduction='mean'):
        super().__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
        self.reduction = reduction
    
    def forward(self, pred, target):
        # pred: [batch_size, num_classes] (logits)
        # target: [batch_size] (class indices)
        
        log_prob = F.log_softmax(pred, dim=-1)
        
        # åˆ›å»ºå¹³æ»‘çš„ç›®æ ‡åˆ†å¸ƒ
        smooth_target = torch.zeros_like(log_prob)
        smooth_target.fill_(self.smoothing / (self.num_classes - 1))
        smooth_target.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)
        
        loss = -smooth_target * log_prob
        
        if self.reduction == 'mean':
            return loss.sum(dim=-1).mean()
        else:
            return loss.sum(dim=-1).sum()

# è‡ªé€‚åº”æ ‡ç­¾å¹³æ»‘
class AdaptiveLabelSmoothing(nn.Module):
    def __init__(self, num_classes, initial_smoothing=0.1):
        super().__init__()
        self.num_classes = num_classes
        self.smoothing = nn.Parameter(torch.tensor(initial_smoothing))
    
    def forward(self, pred, target):
        smoothing = torch.sigmoid(self.smoothing)  # ç¡®ä¿åœ¨[0,1]èŒƒå›´å†…
        
        log_prob = F.log_softmax(pred, dim=-1)
        smooth_target = torch.zeros_like(log_prob)
        smooth_target.fill_(smoothing / (self.num_classes - 1))
        smooth_target.scatter_(1, target.unsqueeze(1), 1.0 - smoothing)
        
        loss = -smooth_target * log_prob
        return loss.sum(dim=-1).mean()
```

---

## ğŸ”„ å››ã€æ’åºä¸åº¦é‡å­¦ä¹ æŸå¤±å‡½æ•°

### 4.1 Triplet Loss

#### æ•°å­¦å®šä¹‰
```
Triplet(a, p, n) = max(0, ||f(a) - f(p)||Â² - ||f(a) - f(n)||Â² + margin)

å…¶ä¸­ï¼š
a: anchor (é”šç‚¹)
p: positive (æ­£æ ·æœ¬)
n: negative (è´Ÿæ ·æœ¬)
```

#### ä»£ç å®ç°
```python
class TripletLoss(nn.Module):
    def __init__(self, margin=1.0, p=2):
        super().__init__()
        self.margin = margin
        self.p = p
    
    def forward(self, anchor, positive, negative):
        distance_positive = F.pairwise_distance(anchor, positive, p=self.p)
        distance_negative = F.pairwise_distance(anchor, negative, p=self.p)
        
        loss = F.relu(distance_positive - distance_negative + self.margin)
        return loss.mean()

# å›°éš¾ä¸‰å…ƒç»„æŒ–æ˜
class TripletLossWithHardMining(nn.Module):
    def __init__(self, margin=1.0, hard_mining=True):
        super().__init__()
        self.margin = margin
        self.hard_mining = hard_mining
    
    def forward(self, embeddings, labels):
        # è®¡ç®—æ‰€æœ‰æ ·æœ¬å¯¹ä¹‹é—´çš„è·ç¦»
        distances = torch.cdist(embeddings, embeddings)
        
        batch_size = embeddings.size(0)
        triplet_losses = []
        
        for i in range(batch_size):
            anchor_label = labels[i]
            
            # æ‰¾åˆ°æ­£æ ·æœ¬ï¼ˆç›¸åŒæ ‡ç­¾ï¼‰
            positive_mask = (labels == anchor_label) & (torch.arange(batch_size) != i)
            positive_distances = distances[i][positive_mask]
            
            if len(positive_distances) == 0:
                continue
            
            # æ‰¾åˆ°è´Ÿæ ·æœ¬ï¼ˆä¸åŒæ ‡ç­¾ï¼‰
            negative_mask = labels != anchor_label
            negative_distances = distances[i][negative_mask]
            
            if len(negative_distances) == 0:
                continue
            
            if self.hard_mining:
                # å›°éš¾æ­£æ ·æœ¬ï¼šè·ç¦»æœ€è¿œçš„æ­£æ ·æœ¬
                hardest_positive = positive_distances.max()
                # å›°éš¾è´Ÿæ ·æœ¬ï¼šè·ç¦»æœ€è¿‘çš„è´Ÿæ ·æœ¬
                hardest_negative = negative_distances.min()
                
                loss = F.relu(hardest_positive - hardest_negative + self.margin)
                triplet_losses.append(loss)
            else:
                # æ‰€æœ‰å¯èƒ½çš„ä¸‰å…ƒç»„
                for pos_dist in positive_distances:
                    for neg_dist in negative_distances:
                        loss = F.relu(pos_dist - neg_dist + self.margin)
                        triplet_losses.append(loss)
        
        if len(triplet_losses) == 0:
            return torch.tensor(0.0, requires_grad=True)
        
        return torch.stack(triplet_losses).mean()
```

### 4.2 Contrastive Loss

#### æ•°å­¦å®šä¹‰
```
Contrastive(x1, x2, y) = {
    DÂ²                    if y = 1 (ç›¸ä¼¼)
    max(0, margin - D)Â²   if y = 0 (ä¸ç›¸ä¼¼)
}

å…¶ä¸­D = ||f(x1) - f(x2)||
```

#### ä»£ç å®ç°
```python
class ContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super().__init__()
        self.margin = margin
    
    def forward(self, output1, output2, label):
        euclidean_distance = F.pairwise_distance(output1, output2)
        
        # ç›¸ä¼¼å¯¹çš„æŸå¤±
        loss_positive = label * euclidean_distance.pow(2)
        
        # ä¸ç›¸ä¼¼å¯¹çš„æŸå¤±
        loss_negative = (1 - label) * F.relu(self.margin - euclidean_distance).pow(2)
        
        loss = loss_positive + loss_negative
        return loss.mean()

# æ¸©åº¦ç¼©æ”¾å¯¹æ¯”å­¦ä¹ æŸå¤±
class TemperatureScaledContrastiveLoss(nn.Module):
    def __init__(self, temperature=0.1):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, features, labels):
        batch_size = features.size(0)
        
        # å½’ä¸€åŒ–ç‰¹å¾
        features = F.normalize(features, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼æ€§çŸ©é˜µ
        similarity_matrix = torch.matmul(features, features.T) / self.temperature
        
        # åˆ›å»ºæ ‡ç­¾æ©ç 
        labels = labels.unsqueeze(1)
        positive_mask = (labels == labels.T).float()
        
        # ç§»é™¤å¯¹è§’çº¿ï¼ˆè‡ªå·±ä¸è‡ªå·±çš„ç›¸ä¼¼æ€§ï¼‰
        positive_mask.fill_diagonal_(0)
        
        # è®¡ç®—æŸå¤±
        exp_sim = torch.exp(similarity_matrix)
        sum_exp_sim = exp_sim.sum(dim=1, keepdim=True)
        
        log_prob = similarity_matrix - torch.log(sum_exp_sim)
        loss = -(positive_mask * log_prob).sum(dim=1) / positive_mask.sum(dim=1).clamp(min=1)
        
        return loss.mean()
```

---

## ğŸ¨ äº”ã€ç”Ÿæˆä»»åŠ¡æŸå¤±å‡½æ•°

### 5.1 å¯¹æŠ—æŸå¤± (Adversarial Loss)

#### åŸºæœ¬GANæŸå¤±
```python
class GANLoss(nn.Module):
    def __init__(self, gan_mode='vanilla', target_real_label=1.0, target_fake_label=0.0):
        super().__init__()
        self.register_buffer('real_label', torch.tensor(target_real_label))
        self.register_buffer('fake_label', torch.tensor(target_fake_label))
        self.gan_mode = gan_mode
        
        if gan_mode == 'lsgan':
            self.loss = nn.MSELoss()
        elif gan_mode == 'vanilla':
            self.loss = nn.BCEWithLogitsLoss()
        elif gan_mode == 'wgangp':
            self.loss = None
    
    def get_target_tensor(self, prediction, target_is_real):
        if target_is_real:
            target_tensor = self.real_label
        else:
            target_tensor = self.fake_label
        return target_tensor.expand_as(prediction)
    
    def forward(self, prediction, target_is_real):
        if self.gan_mode in ['lsgan', 'vanilla']:
            target_tensor = self.get_target_tensor(prediction, target_is_real)
            loss = self.loss(prediction, target_tensor)
        elif self.gan_mode == 'wgangp':
            if target_is_real:
                loss = -prediction.mean()
            else:
                loss = prediction.mean()
        
        return loss

# WGAN-GPæ¢¯åº¦æƒ©ç½š
class GradientPenalty(nn.Module):
    def __init__(self, lambda_gp=10.0):
        super().__init__()
        self.lambda_gp = lambda_gp
    
    def forward(self, discriminator, real_data, fake_data):
        batch_size = real_data.size(0)
        
        # éšæœºæ’å€¼
        epsilon = torch.rand(batch_size, 1, 1, 1).to(real_data.device)
        epsilon = epsilon.expand_as(real_data)
        
        interpolated = epsilon * real_data + (1 - epsilon) * fake_data
        interpolated.requires_grad_(True)
        
        # è®¡ç®—åˆ¤åˆ«å™¨è¾“å‡º
        d_interpolated = discriminator(interpolated)
        
        # è®¡ç®—æ¢¯åº¦
        gradients = torch.autograd.grad(
            outputs=d_interpolated,
            inputs=interpolated,
            grad_outputs=torch.ones(d_interpolated.size()).to(real_data.device),
            create_graph=True,
            retain_graph=True,
            only_inputs=True
        )[0]
        
        # è®¡ç®—æ¢¯åº¦èŒƒæ•°
        gradients = gradients.view(batch_size, -1)
        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
        
        return self.lambda_gp * gradient_penalty
```

### 5.2 æ„ŸçŸ¥æŸå¤± (Perceptual Loss)

#### ä»£ç å®ç°
```python
import torchvision.models as models

class PerceptualLoss(nn.Module):
    def __init__(self, feature_layers=[3, 8, 15, 22], weights=[1.0, 1.0, 1.0, 1.0]):
        super().__init__()
        
        # ä½¿ç”¨é¢„è®­ç»ƒçš„VGGç½‘ç»œ
        vgg = models.vgg16(pretrained=True).features
        self.feature_extractor = nn.ModuleList()
        
        # åˆ†å‰²VGGç½‘ç»œåˆ°æŒ‡å®šå±‚
        current_layer = 0
        temp_sequential = nn.Sequential()
        
        for i, layer in enumerate(vgg):
            temp_sequential.add_module(str(i), layer)
            
            if i in feature_layers:
                self.feature_extractor.append(temp_sequential)
                temp_sequential = nn.Sequential()
        
        # å†»ç»“å‚æ•°
        for param in self.parameters():
            param.requires_grad = False
        
        self.weights = weights
        self.criterion = nn.MSELoss()
    
    def forward(self, generated, target):
        # é¢„å¤„ç†ï¼šå½’ä¸€åŒ–åˆ°ImageNetçš„å‡å€¼å’Œæ–¹å·®
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(generated.device)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(generated.device)
        
        generated_norm = (generated - mean) / std
        target_norm = (target - mean) / std
        
        loss = 0
        x_gen = generated_norm
        x_target = target_norm
        
        for i, extractor in enumerate(self.feature_extractor):
            x_gen = extractor(x_gen)
            x_target = extractor(x_target)
            
            loss += self.weights[i] * self.criterion(x_gen, x_target)
        
        return loss

# é£æ ¼æŸå¤±ï¼ˆGramçŸ©é˜µï¼‰
class StyleLoss(nn.Module):
    def __init__(self):
        super().__init__()
        self.criterion = nn.MSELoss()
    
    def gram_matrix(self, x):
        batch_size, channels, height, width = x.size()
        features = x.view(batch_size, channels, height * width)
        gram = torch.bmm(features, features.transpose(1, 2))
        return gram / (channels * height * width)
    
    def forward(self, generated_features, target_features):
        generated_gram = self.gram_matrix(generated_features)
        target_gram = self.gram_matrix(target_features)
        return self.criterion(generated_gram, target_gram)
```

---

## ğŸ¯ å…­ã€ç‰¹æ®Šä»»åŠ¡æŸå¤±å‡½æ•°

### 6.1 Dice Loss (å›¾åƒåˆ†å‰²)

#### æ•°å­¦å®šä¹‰
```
Dice = 1 - (2 Ã— |A âˆ© B|) / (|A| + |B|)

å…¶ä¸­Aå’ŒBåˆ†åˆ«ä¸ºé¢„æµ‹å’ŒçœŸå®çš„åˆ†å‰²æ©ç 
```

#### ä»£ç å®ç°
```python
class DiceLoss(nn.Module):
    def __init__(self, smooth=1.0, reduction='mean'):
        super().__init__()
        self.smooth = smooth
        self.reduction = reduction
    
    def forward(self, pred, target):
        # å±•å¹³å¼ é‡
        pred = pred.view(-1)
        target = target.view(-1)
        
        # è®¡ç®—äº¤é›†å’Œå¹¶é›†
        intersection = (pred * target).sum()
        dice_coeff = (2.0 * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)
        
        loss = 1 - dice_coeff
        return loss

# å¤šç±»åˆ«Dice Loss
class MultiClassDiceLoss(nn.Module):
    def __init__(self, num_classes, weights=None, smooth=1.0):
        super().__init__()
        self.num_classes = num_classes
        self.weights = weights if weights is not None else torch.ones(num_classes)
        self.smooth = smooth
    
    def forward(self, pred, target):
        # pred: [batch_size, num_classes, height, width]
        # target: [batch_size, height, width]
        
        pred_softmax = F.softmax(pred, dim=1)
        target_onehot = F.one_hot(target, self.num_classes).permute(0, 3, 1, 2).float()
        
        total_loss = 0
        for c in range(self.num_classes):
            pred_c = pred_softmax[:, c]
            target_c = target_onehot[:, c]
            
            intersection = (pred_c * target_c).sum()
            dice_coeff = (2.0 * intersection + self.smooth) / \
                        (pred_c.sum() + target_c.sum() + self.smooth)
            
            loss_c = 1 - dice_coeff
            total_loss += self.weights[c] * loss_c
        
        return total_loss / self.num_classes
```

### 6.2 IoU Loss (ç›®æ ‡æ£€æµ‹)

#### ä»£ç å®ç°
```python
class IoULoss(nn.Module):
    def __init__(self, reduction='mean'):
        super().__init__()
        self.reduction = reduction
    
    def forward(self, pred_boxes, target_boxes):
        """
        pred_boxes, target_boxes: [N, 4] (x1, y1, x2, y2)
        """
        
        # è®¡ç®—äº¤é›†åŒºåŸŸ
        inter_x1 = torch.max(pred_boxes[:, 0], target_boxes[:, 0])
        inter_y1 = torch.max(pred_boxes[:, 1], target_boxes[:, 1])
        inter_x2 = torch.min(pred_boxes[:, 2], target_boxes[:, 2])
        inter_y2 = torch.min(pred_boxes[:, 3], target_boxes[:, 3])
        
        inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * \
                    torch.clamp(inter_y2 - inter_y1, min=0)
        
        # è®¡ç®—å„è‡ªé¢ç§¯
        pred_area = (pred_boxes[:, 2] - pred_boxes[:, 0]) * \
                   (pred_boxes[:, 3] - pred_boxes[:, 1])
        target_area = (target_boxes[:, 2] - target_boxes[:, 0]) * \
                     (target_boxes[:, 3] - target_boxes[:, 1])
        
        # è®¡ç®—å¹¶é›†
        union_area = pred_area + target_area - inter_area
        
        # è®¡ç®—IoU
        iou = inter_area / (union_area + 1e-6)
        
        # IoU Loss
        loss = 1 - iou
        
        if self.reduction == 'mean':
            return loss.mean()
        else:
            return loss.sum()

# GIoU Loss (Generalized IoU)
class GIoULoss(nn.Module):
    def forward(self, pred_boxes, target_boxes):
        # è®¡ç®—IoU
        iou_loss = IoULoss(reduction='none')
        iou = 1 - iou_loss(pred_boxes, target_boxes)
        
        # è®¡ç®—æœ€å°åŒ…å›´æ¡†
        enclose_x1 = torch.min(pred_boxes[:, 0], target_boxes[:, 0])
        enclose_y1 = torch.min(pred_boxes[:, 1], target_boxes[:, 1])
        enclose_x2 = torch.max(pred_boxes[:, 2], target_boxes[:, 2])
        enclose_y2 = torch.max(pred_boxes[:, 3], target_boxes[:, 3])
        
        enclose_area = (enclose_x2 - enclose_x1) * (enclose_y2 - enclose_y1)
        
        # è®¡ç®—å¹¶é›†é¢ç§¯
        pred_area = (pred_boxes[:, 2] - pred_boxes[:, 0]) * \
                   (pred_boxes[:, 3] - pred_boxes[:, 1])
        target_area = (target_boxes[:, 2] - target_boxes[:, 0]) * \
                     (target_boxes[:, 3] - target_boxes[:, 1])
        
        inter_area = iou * (pred_area + target_area) / (1 + iou)
        union_area = pred_area + target_area - inter_area
        
        # GIoU
        giou = iou - (enclose_area - union_area) / enclose_area
        
        return 1 - giou.mean()
```

### 6.3 CTC Loss (åºåˆ—æ ‡æ³¨)

#### ä»£ç å®ç°
```python
class CTCLoss(nn.Module):
    def __init__(self, blank=0, reduction='mean', zero_infinity=False):
        super().__init__()
        self.ctc_loss = nn.CTCLoss(blank=blank, reduction=reduction, zero_infinity=zero_infinity)
    
    def forward(self, log_probs, targets, input_lengths, target_lengths):
        """
        log_probs: [T, N, C] - æ—¶é—´æ­¥Ã—æ‰¹æ¬¡Ã—ç±»åˆ«æ•°ï¼ˆå¯¹æ•°æ¦‚ç‡ï¼‰
        targets: [sum(target_lengths)] - ç›®æ ‡åºåˆ—ï¼ˆå±•å¹³ï¼‰
        input_lengths: [N] - æ¯ä¸ªæ ·æœ¬çš„è¾“å…¥é•¿åº¦
        target_lengths: [N] - æ¯ä¸ªæ ·æœ¬çš„ç›®æ ‡é•¿åº¦
        """
        return self.ctc_loss(log_probs, targets, input_lengths, target_lengths)

# ä½¿ç”¨ç¤ºä¾‹
def ctc_loss_example():
    # æ¨¡æ‹Ÿæ•°æ®
    T, N, C = 50, 16, 20  # æ—¶é—´æ­¥ï¼Œæ‰¹æ¬¡å¤§å°ï¼Œè¯æ±‡è¡¨å¤§å°
    
    # æ¨¡å‹è¾“å‡ºï¼ˆéœ€è¦æ˜¯logæ¦‚ç‡ï¼‰
    log_probs = torch.randn(T, N, C).log_softmax(2)
    
    # ç›®æ ‡åºåˆ—é•¿åº¦
    target_lengths = torch.randint(1, 15, (N,))
    
    # è¾“å…¥é•¿åº¦
    input_lengths = torch.full((N,), T, dtype=torch.long)
    
    # ç›®æ ‡åºåˆ—ï¼ˆå±•å¹³ï¼‰
    targets = torch.randint(1, C, (target_lengths.sum(),))
    
    # è®¡ç®—æŸå¤±
    criterion = CTCLoss(blank=0)
    loss = criterion(log_probs, targets, input_lengths, target_lengths)
    
    return loss
```

---

## ğŸ“‹ ä¸ƒã€æŸå¤±å‡½æ•°é€‰æ‹©æŒ‡å—

### 7.1 ä»»åŠ¡ç±»å‹æ˜ å°„è¡¨

| ä»»åŠ¡ç±»å‹ | æ¨èæŸå¤±å‡½æ•° | å¤‡é€‰æ–¹æ¡ˆ | æ³¨æ„äº‹é¡¹ |
|----------|-------------|----------|----------|
| **å›å½’** | MSE | Huber, MAE | å¼‚å¸¸å€¼é€‰æ‹©Huber/MAE |
| **äºŒåˆ†ç±»** | BCE | Focal Loss | ä¸å¹³è¡¡æ•°æ®ç”¨Focal |
| **å¤šåˆ†ç±»** | Cross-Entropy | Label Smoothing | è¿‡æ‹Ÿåˆæ—¶ç”¨Label Smoothing |
| **å¤šæ ‡ç­¾** | BCE per Label | - | æ¯ä¸ªæ ‡ç­¾ç‹¬ç«‹é¢„æµ‹ |
| **å›¾åƒåˆ†å‰²** | Cross-Entropy + Dice | IoU Loss | ç»„åˆä½¿ç”¨æ•ˆæœæ›´å¥½ |
| **ç›®æ ‡æ£€æµ‹** | IoU Loss | GIoU, DIoU | æ¡†å›å½’ç”¨IoUç³»åˆ— |
| **ç”Ÿæˆä»»åŠ¡** | Adversarial + MSE | Perceptual Loss | è´¨é‡è¦æ±‚é«˜ç”¨Perceptual |
| **åºåˆ—æ ‡æ³¨** | CTC | - | åºåˆ—å¯¹é½é—®é¢˜ä¸“ç”¨ |
| **åº¦é‡å­¦ä¹ ** | Triplet Loss | Contrastive | æ ·æœ¬å…³ç³»å­¦ä¹  |

### 7.2 è‡ªåŠ¨æŸå¤±å‡½æ•°é€‰æ‹©å™¨

```python
class LossFunctionSelector:
    def __init__(self):
        self.task_loss_mapping = {
            'regression': self._select_regression_loss,
            'binary_classification': self._select_binary_loss,
            'multi_classification': self._select_multi_loss,
            'segmentation': self._select_segmentation_loss,
            'detection': self._select_detection_loss,
            'generation': self._select_generation_loss
        }
    
    def select_loss(self, task_type, data_characteristics):
        """
        æ ¹æ®ä»»åŠ¡ç±»å‹å’Œæ•°æ®ç‰¹å¾è‡ªåŠ¨é€‰æ‹©æŸå¤±å‡½æ•°
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹
            data_characteristics: æ•°æ®ç‰¹å¾å­—å…¸
        """
        if task_type in self.task_loss_mapping:
            return self.task_loss_mapping[task_type](data_characteristics)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {task_type}")
    
    def _select_regression_loss(self, characteristics):
        has_outliers = characteristics.get('has_outliers', False)
        noise_level = characteristics.get('noise_level', 'medium')
        
        if has_outliers or noise_level == 'high':
            return HuberLoss(delta=1.0)
        elif noise_level == 'low':
            return nn.MSELoss()
        else:
            return nn.L1Loss()
    
    def _select_binary_loss(self, characteristics):
        is_imbalanced = characteristics.get('is_imbalanced', False)
        imbalance_ratio = characteristics.get('imbalance_ratio', 1.0)
        
        if is_imbalanced and imbalance_ratio > 10:
            return FocalLoss(alpha=0.25, gamma=2)
        elif is_imbalanced:
            pos_weight = torch.tensor(imbalance_ratio)
            return nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        else:
            return nn.BCEWithLogitsLoss()
    
    def _select_multi_loss(self, characteristics):
        num_classes = characteristics.get('num_classes', 10)
        is_imbalanced = characteristics.get('is_imbalanced', False)
        has_label_noise = characteristics.get('has_label_noise', False)
        
        if has_label_noise:
            return LabelSmoothingLoss(num_classes, smoothing=0.1)
        elif is_imbalanced:
            return MultiClassFocalLoss(gamma=2)
        else:
            return nn.CrossEntropyLoss()
    
    def _select_segmentation_loss(self, characteristics):
        is_imbalanced = characteristics.get('is_imbalanced', False)
        
        if is_imbalanced:
            # ç»„åˆæŸå¤±ï¼šäº¤å‰ç†µ + Dice
            return CombinedLoss([
                nn.CrossEntropyLoss(),
                MultiClassDiceLoss(characteristics['num_classes'])
            ], weights=[0.5, 0.5])
        else:
            return nn.CrossEntropyLoss()

# ç»„åˆæŸå¤±å‡½æ•°
class CombinedLoss(nn.Module):
    def __init__(self, loss_functions, weights):
        super().__init__()
        self.loss_functions = loss_functions
        self.weights = weights
    
    def forward(self, pred, target):
        total_loss = 0
        for loss_fn, weight in zip(self.loss_functions, self.weights):
            total_loss += weight * loss_fn(pred, target)
        return total_loss
```

### 7.3 æŸå¤±å‡½æ•°æ€§èƒ½è¯„ä¼°

```python
class LossEvaluator:
    def __init__(self, model, train_loader, val_loader, num_epochs=5):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.num_epochs = num_epochs
    
    def evaluate_loss_function(self, loss_fn, optimizer_class=torch.optim.Adam):
        """è¯„ä¼°å•ä¸ªæŸå¤±å‡½æ•°çš„æ€§èƒ½"""
        
        model_copy = copy.deepcopy(self.model)
        optimizer = optimizer_class(model_copy.parameters(), lr=1e-3)
        
        train_losses = []
        val_losses = []
        
        for epoch in range(self.num_epochs):
            # è®­ç»ƒ
            model_copy.train()
            train_loss = 0
            for data, target in self.train_loader:
                optimizer.zero_grad()
                output = model_copy(data)
                loss = loss_fn(output, target)
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
            
            train_losses.append(train_loss / len(self.train_loader))
            
            # éªŒè¯
            model_copy.eval()
            val_loss = 0
            with torch.no_grad():
                for data, target in self.val_loader:
                    output = model_copy(data)
                    loss = loss_fn(output, target)
                    val_loss += loss.item()
            
            val_losses.append(val_loss / len(self.val_loader))
        
        # è¯„ä¼°æŒ‡æ ‡
        final_train_loss = train_losses[-1]
        final_val_loss = val_losses[-1]
        convergence_speed = train_losses[0] - train_losses[-1]
        stability = 1.0 / (np.std(val_losses) + 1e-8)
        
        return {
            'final_train_loss': final_train_loss,
            'final_val_loss': final_val_loss,
            'convergence_speed': convergence_speed,
            'stability': stability,
            'train_losses': train_losses,
            'val_losses': val_losses
        }
    
    def compare_loss_functions(self, loss_functions):
        """æ¯”è¾ƒå¤šä¸ªæŸå¤±å‡½æ•°"""
        results = {}
        
        for name, loss_fn in loss_functions.items():
            print(f"è¯„ä¼°æŸå¤±å‡½æ•°: {name}")
            result = self.evaluate_loss_function(loss_fn)
            results[name] = result
        
        # å¯è§†åŒ–æ¯”è¾ƒ
        self.plot_comparison(results)
        
        return results
    
    def plot_comparison(self, results):
        """å¯è§†åŒ–æŸå¤±å‡½æ•°æ¯”è¾ƒç»“æœ"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # è®­ç»ƒæŸå¤±æ›²çº¿
        for name, result in results.items():
            axes[0, 0].plot(result['train_losses'], label=name)
        axes[0, 0].set_title('Training Loss')
        axes[0, 0].legend()
        
        # éªŒè¯æŸå¤±æ›²çº¿
        for name, result in results.items():
            axes[0, 1].plot(result['val_losses'], label=name)
        axes[0, 1].set_title('Validation Loss')
        axes[0, 1].legend()
        
        # æœ€ç»ˆæ€§èƒ½æ¯”è¾ƒ
        names = list(results.keys())
        final_losses = [results[name]['final_val_loss'] for name in names]
        axes[1, 0].bar(names, final_losses)
        axes[1, 0].set_title('Final Validation Loss')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # æ”¶æ•›é€Ÿåº¦æ¯”è¾ƒ
        convergence_speeds = [results[name]['convergence_speed'] for name in names]
        axes[1, 1].bar(names, convergence_speeds)
        axes[1, 1].set_title('Convergence Speed')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.show()
```

---

## ğŸ¯ å…«ã€æœ€ä½³å®è·µæ€»ç»“

### 8.1 æŸå¤±å‡½æ•°é€‰æ‹©æ¸…å•

**âœ… åŸºç¡€æ£€æŸ¥**ï¼š
- [ ] ä»»åŠ¡ç±»å‹æ˜¯å¦åŒ¹é…æŸå¤±å‡½æ•°
- [ ] æ•°æ®åˆ†å¸ƒæ˜¯å¦å¹³è¡¡
- [ ] æ˜¯å¦å­˜åœ¨å¼‚å¸¸å€¼
- [ ] æ ‡ç­¾è´¨é‡å¦‚ä½•

**âœ… å®ç°æ£€æŸ¥**ï¼š
- [ ] æ•°å€¼ç¨³å®šæ€§ï¼ˆé¿å…log(0)ï¼‰
- [ ] æ¢¯åº¦è®¡ç®—æ­£ç¡®æ€§
- [ ] å†…å­˜ä½¿ç”¨æ•ˆç‡
- [ ] è®¡ç®—é€Ÿåº¦

**âœ… è®­ç»ƒç›‘æ§**ï¼š
- [ ] æŸå¤±æ›²çº¿æ˜¯å¦åˆç†
- [ ] æ˜¯å¦å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
- [ ] æ”¶æ•›é€Ÿåº¦æ˜¯å¦æ»¡è¶³è¦æ±‚
- [ ] éªŒè¯é›†è¡¨ç°

### 8.2 å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

```python
# é—®é¢˜è¯Šæ–­å·¥å…·
class LossDiagnostics:
    @staticmethod
    def diagnose_loss_curve(train_losses, val_losses):
        """è¯Šæ–­æŸå¤±æ›²çº¿å¼‚å¸¸"""
        issues = []
        
        # æ£€æŸ¥æ”¶æ•›æ€§
        if len(train_losses) > 10:
            recent_trend = np.polyfit(range(len(train_losses[-10:])), train_losses[-10:], 1)[0]
            if recent_trend > -0.001:
                issues.append("è®­ç»ƒæŸå¤±å¯èƒ½å·²ç»åœæ­¢ä¸‹é™")
        
        # æ£€æŸ¥è¿‡æ‹Ÿåˆ
        if len(val_losses) > 5:
            train_final = train_losses[-5:]
            val_final = val_losses[-5:]
            
            if np.mean(val_final) > np.mean(train_final) * 1.5:
                issues.append("å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ")
        
        # æ£€æŸ¥ä¸ç¨³å®šæ€§
        if len(val_losses) > 10:
            val_std = np.std(val_losses[-10:])
            if val_std > np.mean(val_losses[-10:]) * 0.1:
                issues.append("éªŒè¯æŸå¤±ä¸ç¨³å®š")
        
        return issues if issues else ["æŸå¤±æ›²çº¿æ­£å¸¸"]
    
    @staticmethod
    def check_gradient_health(model):
        """æ£€æŸ¥æ¢¯åº¦å¥åº·çŠ¶å†µ"""
        total_norm = 0
        param_count = 0
        
        for param in model.parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1
        
        total_norm = total_norm ** (1. / 2)
        
        if total_norm > 100:
            return "æ¢¯åº¦è¿‡å¤§ï¼Œå¯èƒ½å‘ç”Ÿæ¢¯åº¦çˆ†ç‚¸"
        elif total_norm < 1e-7:
            return "æ¢¯åº¦è¿‡å°ï¼Œå¯èƒ½å‘ç”Ÿæ¢¯åº¦æ¶ˆå¤±"
        else:
            return f"æ¢¯åº¦æ­£å¸¸ï¼ŒèŒƒæ•°: {total_norm:.4f}"
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- **é‡å­ä¼˜åŒ–**: [[K1-åŸºç¡€ç†è®ºä¸æ¦‚å¿µ/è®¡ç®—åŸºç¡€/é‡å­è®¡ç®—é¿å…å±€éƒ¨æœ€ä¼˜ï¼šåŸç†ã€æŒ‘æˆ˜ä¸AIåº”ç”¨å‰æ²¿|é‡å­è®¡ç®—é¿å…å±€éƒ¨æœ€ä¼˜ï¼šåŸç†ã€æŒ‘æˆ˜ä¸AIåº”ç”¨å‰æ²¿]]
- **Losså‡½æ•°è°ƒä¼˜**: [[K2-æŠ€æœ¯æ–¹æ³•ä¸å®ç°/è®­ç»ƒæŠ€æœ¯/Losså‡½æ•°ä¸æ¨¡å‹è°ƒä¼˜å…¨é¢æŒ‡å—|Losså‡½æ•°ä¸æ¨¡å‹è°ƒä¼˜å…¨é¢æŒ‡å—]]
- **ä¼˜åŒ–å™¨ç®—æ³•**: [[K2-æŠ€æœ¯æ–¹æ³•ä¸å®ç°/ä¼˜åŒ–æ–¹æ³•/æ·±åº¦å­¦ä¹ ä¼˜åŒ–å™¨ç®—æ³•å¯¹æ¯”åˆ†æ|æ·±åº¦å­¦ä¹ ä¼˜åŒ–å™¨ç®—æ³•å¯¹æ¯”åˆ†æ]]
- **æ­£åˆ™åŒ–æŠ€æœ¯**: [[K2-æŠ€æœ¯æ–¹æ³•ä¸å®ç°/ä¼˜åŒ–æ–¹æ³•/æ·±åº¦å­¦ä¹ æ­£åˆ™åŒ–æŠ€æœ¯å…¨é¢æŒ‡å—|æ·±åº¦å­¦ä¹ æ­£åˆ™åŒ–æŠ€æœ¯å…¨é¢æŒ‡å—]]
- **Hugging Faceç”Ÿæ€**: [[K3-å·¥å…·å¹³å°ä¸ç”Ÿæ€/å¼€å‘å¹³å°/Hugging Faceç”Ÿæ€å…¨é¢æŒ‡å—|Hugging Faceç”Ÿæ€å…¨é¢æŒ‡å—]]

---

**æ›´æ–°æ—¶é—´**: 2025å¹´1æœˆ  
**ç»´æŠ¤è€…**: AIçŸ¥è¯†åº“å›¢é˜Ÿ  
**éš¾åº¦è¯„çº§**: â­â­â­â­ (éœ€è¦æ·±åº¦å­¦ä¹ ç†è®ºåŸºç¡€å’Œå®è·µç»éªŒ)
