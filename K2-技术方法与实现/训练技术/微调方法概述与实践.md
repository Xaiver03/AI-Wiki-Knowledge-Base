# **1. 指令监督微调（[[SFT（Supervised Fine-Tuning，监督微调）|SFT]]）**

# **1. 指令监督微调（[[SFT（Supervised Fine-Tuning，监督微调）|SFT]]）**
> **关联**：[[K1-基础理论与概念/核心概念/损失函数与训练调优术语名词库|术语名词库（大白话对照）]]

[[SFT（Supervised Fine-Tuning，监督微调）|SFT]]是传统的微调方法，利用带有明确指令和预期输出的标注数据，通过有监督学习调整模型参数，使其更好地完成特定任务。

- **优势**：
    
    - 适用于结构化任务，如问答、翻译等。
        
    - 训练过程稳定，易于实现。
- **局限性**：
    
    - 高质量标注数据获取成本高。
        
    - 难以捕捉多样化的解决方案。
        
    - 优化目标可能与实际需求不一致。

# **2. 强化学习（RL）与基于人类反馈的强化学习（[[RLHF人类反馈强化学习|RLHF]]）**

[[RLHF人类反馈强化学习|RLHF]]通过引入奖励机制，使模型在生成输出时能够根据反馈信号进行优化，从而更好地满足人类的期望。

- **优势**：
    
    - 直接优化模型的行为，使其更符合人类偏好。
        
    - 无需大量标注数据。
        
    - 适合处理复杂的推理和决策任务。
- **挑战**：
    
    - 设计合适的奖励函数较为复杂。
        
    - 训练过程可能不稳定，需要精心调试。

---

# **🔧 技术实现细节**

# **1. 轻量级微调（PEFT）**

PEFT（Parameter-Efficient Fine-Tuning）是一种高效的微调方法，通过冻结大部分预训练参数，仅更新少量参数，实现模型的快速适应。常见的PEFT方法包括[[LoRA低秩适应微调|LoRA]]、QLoRA、DoRA等。

- **优点**：
    
    - 降低计算资源需求。
        
    - 适合在资源受限的环境中部署。

# **2. 数据格式**

在SFT中，常用的数据格式包括Alpaca和ShareGPT格式，结构清晰，便于模型学习。

- **Alpaca格式**：

```
{
  "instruction": "Translate the following English text to French.",
  "input": "Hello, how are you?",
  "output": "Bonjour, comment ça va?"
}
```

-   
    
- **ShareGPT格式**：

```
[
  {
    "conversation": [
      {
        "system": "You are an AI assistant.",
        "input": "Hello?",
        "output": "Hello! How can I help you?"
      },
      {
        "input": "What's the date today?",
        "output": "Today is Monday, August 14, 2023."
      }
    ]
  }
]
```

在RLHF中，数据格式通常包括提示（prompt）、被拒绝的输出（rejected）和被选择的输出（chosen），用于训练奖励模型。

- **DPO/PPO格式**：

```
{
  "prompt": "Explain the theory of relativity.",
  "rejected": "It's a theory about time.",
  "chosen": "The theory of relativity, developed by Albert Einstein, encompasses two interrelated theories..."
}
```

---

# **📚 学习路径建议**

文章还提供了一个系统的学习路径，帮助读者从初级到高级逐步掌握大模型的微调技术：

1. **初阶应用（10天）**：了解大模型的基本概念和应用场景。
    
2. **高阶应用（30天）**：学习构建私有知识库，掌握RAG（Retrieval-Augmented Generation）技术。
    
3. **模型训练（30天）**：深入理解模型训练流程，掌握微调和轻量化微调技术。
    
4. **商业闭环（20天）**：学习模型部署和商业化应用，构建完整的AI解决方案。
