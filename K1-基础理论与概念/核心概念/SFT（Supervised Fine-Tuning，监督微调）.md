🏷 #训练 #NLP #进阶

# **1. 概念**

SFT（Supervised Fine-Tuning，监督微调）是[[Post-Training后训练技术]]中的重要方法之一，也是大语言模型（LLM）训练流程中的重要环节。它指的是在**大规模预训练模型**（如[[Transformer架构原理|Transformer]]架构）的基础上，利用**人工标注的监督数据集**（通常是高质量的"指令-回答"对）来进一步微调模型，使其更符合人类意图或特定任务的需求。

简单来说：

- **预训练**：模型通过大规模文本数据学习语言模式（无监督/自监督）。
    
- **SFT**：在此基础上，用人工整理的高质量数据对模型进行“矫正”和“定向”，让模型学会按照指令输出更合适的结果。

---

# **2. 流程**

SFT 一般处于 LLM 对齐（Alignment）阶段的第一步，整体流程如下：

```mermaid
flowchart LR
A[预训练模型] --> B[SFT（人工标注数据微调）] --> C[RLHF/DPO 等对齐方法] --> D[最终对齐模型]
```

**具体步骤：**

1. **准备数据**：收集高质量的“指令-回答”数据，通常由人工标注。
    
2. **监督训练**：用这些数据对预训练模型进行微调。
    
3. **模型输出优化**：模型在此阶段会学会更符合人类语言习惯和预期的回答。
    
4. **进一步对齐**：再结合 [[RLHF人类反馈强化学习]] 或 [[DPO直接偏好优化]] 等方法提升效果。

---

# **3. 举例**

- **ChatGPT 的训练**：在 GPT-3 预训练的基础上，先进行 **SFT**（用人工写的问答数据），再用 **[[RLHF人类反馈强化学习]]** 调整。
    
- **垂直场景应用**：比如在医疗、金融、法律领域，通过 SFT 注入领域内的专家数据，让模型更适应专业任务。

---

# **4. 优点**

- **快速定向**：相比重新预训练，只需较少的数据和计算量。
    
- **效果显著**：能显著提升模型的实用性和用户体验。
    
- **灵活性**：可针对不同任务或领域进行专门定制。

---

# **5. 局限与批判**

- **数据依赖性强**：需要大量高质量人工标注数据，成本高。
    
- **覆盖有限**：SFT 只能让模型在标注数据覆盖的场景表现更好，遇到新问题仍可能失效。
    
- **可能引入偏差**：人工标注的不均衡和主观性可能放大某些偏见。
    
- **过拟合风险**：如果数据过窄，模型可能丧失通用性。

---

# **6. 总结**

SFT 是大语言模型对齐过程中的关键一环。它通过监督数据让模型具备"遵循指令"的能力，但并不能彻底解决对齐问题，需要结合 [[RLHF人类反馈强化学习]]、[[DPO直接偏好优化]] 等方法形成完整的 **Alignment Pipeline**。
