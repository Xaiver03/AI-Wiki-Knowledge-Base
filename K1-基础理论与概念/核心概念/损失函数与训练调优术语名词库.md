---
tags:
  - 术语
  - 名词库
  - 训练技术
  - 损失函数
---
# 📚 损失函数与训练调优术语名词库（大白话对照）

> 关联：[[K2-技术方法与实现/训练技术/Loss函数与模型调优全面指南|Loss调优指南]]｜[[K2-技术方法与实现/训练技术/损失函数类型全解析：从基础到高级应用|损失函数全解析]]

本名词库统一了训练与损失函数相关的核心术语，提供“大白话 vs 专业解释”的对照，便于非技术读者快速上手，同时为技术读者保留准确表述。

---

## 🧠 术语速查表（大白话 vs 专业解释）

| 术语 | 大白话解释 | 专业解释 |
|---|---|---|
| 损失函数（Loss） | 给模型“打分”的扣分器，分越小越好 | 定义在预测与目标上的非负实值函数，是经验风险或其正则化形式，用于指导参数更新 |
| 目标/代价函数 | 训练时要“尽量变小”的总目标 | 包含任务损失与正则项的优化目标，最小化其期望或经验值 |
| 梯度（Gradient） | 指向“往下走最快”的箭头 | Loss 对参数的偏导数向量，是一阶优化更新方向 |
| 反向传播 | 从结果倒推每层“该背多少锅” | 基于链式法则的梯度高效计算算法 |
| 学习率（LR） | 每一步走多大步 | 优化器更新步长，过大会发散，过小会收敛慢 |
| 优化器（Optimizer） | 把“箭头”变成“走路”的规则 | 将梯度映射为参数更新的算法，如 SGD、Adam、AdamW |
| [[SGD|SGD（随机梯度下降）]]/动量 | 匀速/加点“惯性”的走法 | 随机梯度下降及其带动量版本，减少噪声并加速收敛 |
| [[Adam]]/[[AdamW]] | “自适应调步子”的走法 | 一阶矩与二阶矩自适应估计；AdamW将权重衰减与梯度解耦 |
| 权重衰减（Weight Decay） | “不给参数长胖”的惩罚 | 等价于L2正则或其解耦形式，抑制过拟合，提高泛化 |
| 正则化（L1/L2） | 防止“死记硬背”的约束 | 在目标中加入参数范数惩罚，L1促稀疏，L2促平滑 |
| Dropout | 训练时“抽掉”部分神经元 | 随机失活以减少协同适应，等效集成提高泛化 |
| 批归一化（BN） | 每层先“抹平再计算”更稳 | 对小批量统计量标准化并引入仿射参数，改善梯度传播 |
| 过拟合 | 练习题满分，考试拉胯 | 训练误差低、泛化误差高，常因容量过大/正则不足 |
| 欠拟合 | 书都没背会 | 训练与验证误差都高，偏差过大、模型/特征不足 |
| 早停（Early Stopping） | 验证集不再变好就停 | 基于验证性能的停止准则，防止过拟合 |
| 学习率预热（Warmup） | 前几步小步慢走 | 初期逐步增大学习率以稳定训练、避免梯度不稳 |
| 学习率调度（Scheduler） | 训练中“按节奏”调步子 | 按Step/余弦/OneCycle等策略随时间调整学习率 |
| 梯度裁剪 | “限速”，防止爆炸 | 约束梯度范数/值域，缓解梯度爆炸提高稳定性 |
| 梯度累积 | “攒几步再走”省显存 | 多次反传累积再更新，等效于更大的有效批量 |
| 批大小（Batch Size） | 一口喂多少样本 | 影响梯度噪声、吞吐与泛化；过大易过拟合/欠探索 |
| 轮次（Epoch） | 数据被完整学一遍 | 一次完整遍历训练集的更新周期 |
| 损失地形（Landscape） | 优化要翻越的“山谷图” | 参数空间上Loss的几何结构，决定优化难度与可达解 |
| 泛化（Generalization） | 新题也能做得好 | 训练得到的假设在未见数据上的期望表现 |
| [[MSE|均方误差（MSE）]] | 大错罚得特别重 | 二范数平方的平均值，平滑可微，对离群点敏感 |
| [[MAE|平均绝对误差（MAE）]] | 一视同仁地按“差多少”计 | L1范数的平均值，鲁棒，对异常值不敏感，在0处次可微 |
| Huber Loss | 小错按MSE、大错按MAE | 分段二次/线性损失，通过δ平衡稳健性与可微性 |
| Log-Cosh | Huber的“平滑版” | log(cosh(误差))，处处二阶可微，小误差近似L2，大误差近似L1 |
| 分位数损失（Quantile） | 预测“第几分位”的值 | 不对称线性损失，针对目标分位数τ的回归 |
| [[BCE|二分类交叉熵（BCE）]] | 预测对就奖，错就重罚 | 基于极大似然的负对数似然，对log(0)需数值稳定 |
| [[CE|多分类交叉熵（CE）]] | 选错就扣更多分 | 目标分布与预测分布的交叉熵，常与Softmax配合 |
| 标签平滑（Label Smoothing） | 别太自信，给其他类留点分 | 将one-hot目标平滑为(1-ε, ε/(K-1))分布，起正则化作用 |
| Focal Loss | 把精力放在难样本 | 在交叉熵上加(1-pt)^γ权重抑制易样本、突出难样本 |
| 排序损失（Ranking） | 排名错了就罚 | 面向排序/检索的相对次序学习，如pairwise/listwise |
| 三元组损失（Triplet） | 拉近同类，推远异类 | 约束ap与an距离差+margin，度量学习常用 |
| 对比损失（Contrastive） | 相似靠近，不似分开 | y=1罚距离，y=0罚小于margin的距离，孪生网络常用 |
| Dice Loss | “区域重叠越高越好” | 1−Dice系数，强调小目标/不平衡分割的重叠质量 |
| IoU/GIoU 系列 | 交并比的“可优化版本” | 以IoU及其改进项为目标的检测框回归损失 |
| CTC Loss | 不用逐字对齐也能学 | 在隐式对齐下最大化标签路径概率的序列准则 |
| 对抗损失（GAN） | 生成器和判别器“对打” | 极小极大博弈：生成器最小化、判别器最大化对抗目标 |
| 重构损失（Reconstruction） | 还原得像就加分 | 输入与重建间的L1/L2/CE等度量，VAE/自编码器常用 |
| 感知损失（Perceptual） | 看起来像就行 | 在感知特征空间（如VGG特征）上的距离度量 |
| KL散度（KL-Divergence） | 两个分布“有多不像” | 非对称信息散度，常用于正则化（如VAE, 蒸馏） |
| Logits | 还没过“概率门”的分数 | Softmax/Sigmoid之前的原始输出，数值更稳定 |
| Softmax/Sigmoid | 把分数变概率的门 | 多类归一化/二类映射到(0,1)，用于CE/BCE计算 |
| 类别不平衡 | 好坏人数量差太多 | 频次偏差导致经验风险偏向多数类，需重加权/重采样 |
| 多标签 vs 多分类 | 可多选 vs 只能单选 | 多标签独立二分类，常用BCE；多分类互斥，常用CE |

---

更新时间：2025-09-08  
维护者：AI知识库团队

