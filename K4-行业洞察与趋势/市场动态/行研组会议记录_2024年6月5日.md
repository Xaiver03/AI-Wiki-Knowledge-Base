## 同步Insight

### 关于Google

训练的变化：

- Google新的模型方向 之前SFT RL很多，现在是更接近base model 基础模型follow instruction的能力提高，不断优化instruction，不是按照行业去分的，针对分类从做SFT到instruction
    
- 过去3-6个月有个底层模型更新，需要SFT再跑一遍，现在发现SFT维护成本很高，需要Domain knowledge
    
- 组织管理从5个模型团队 都变成一个团队 每两周一个版本 能用instruct就instruct。 会控制domain往base model的污染。

数据的变化：

- 越来越多的压力从上层下来，就是少用人工标注
    
- 人工工作量太大了，越来越多被push做auto rating
    
- 人工标注human rating
    
    - 一般是 bootstrap 冷启动 或者 release 的时候
        
- 一开始都是手写的用户画像 ，personalization面向所有人 也就几百几千个数据
    
- AI 标注在[[01-核心知识库/K2-技术方法与实现/模型架构/Agent]]场景的使用方法：
    
    - 核心是找到缺失的能力，找1-2个数据有时候就够了
        
    - 用户标注20个数据 个人经验 最多扩大到200个，就足够了，如果项目很重要就最多乘以5
        
- 手写数据构建方法：
    
    - 用户写一堆rubics 创建虚拟用户画像 覆盖到想要覆盖的方面
        
    - 西班牙 老师 孩子 我喜欢干什么 直接让模型去生成 去处理 问是否符合用户画像 10-20个数据就不错了 写一个要大半天
        
    - 有的是用手机去大街上实时去采集 去问 去录屏
        
    - 一个rater需要先写几百个单词的用户画像，在合适的场景 交互 ，5轮对话，结果是这个 我们期望是这个，最后写几百字的总结，费用很高。
        
    - system prompt长了2-3倍
        
    - 数据 150个domian 每个domian几千个data point， 找人写的很好，迭代比较慢
- 专家数据：
    
    - 如何确定评估角度应该有哪些，以及具体的评分标准是否合理
        
    - 组织方面的问题，会写rubric，需要从edge case找每个人的boundary在哪里，去讨论 和 PM开会，需要对行业理解深的去判断

记忆 Personalization：

- System Prompt User Prompt Instruction Prompt
    
- 最近的chat time summary 就是积累用户关于一些主题的
    
- 比如针对用户就保持一份chat summary

（但是如果是个power user 就没有这方面的优化）

其他：

- 模型调用层面 做一些chache ，把大量的数据放在更底层，能consume更多了，从更多chache里 读更多东西，类似更底层的RAG 常识性 还没落地 超过百万上下文
    
- 不用为了数据影响脚步
    
- Google 未采用 Hybrid 或 native sparse attention 是因为他们在 TPU 中已对相关技术进行了硬件优化。

### **Gemini Diffusion 研究**

  **原理与实现方式**

    基本原理：王鑫宇介绍，text generation diffusion 基于非平衡物理学的热化原理，最初应用于图像，通过加噪和反向学习噪声还原图片，应用于文本时，因文本 token 离散，需对 diffusion model 进行改进。

    实现方式：主要有两种，一是将离散文本投影到连续嵌入空间，使其看起来像连续数据；二是改变模型基本架构，定义离散的反向扩散过程。

  **未来发展趋势**

    与 LP 结合：王鑫宇认为，text generation diffusion 未来可与目前成熟的 LP（如 Pre - train language model）结合，利用其生态和技术优势。

    多模态任务：除文本外，还可应用于文本条件的多模态任务，符合其发展逻辑。

  **与其他方法的对比**

    连贯性与错误积累：与普通文本回归相比，text generation diffusion 对词块进行优化，连贯性更强，能避免 next token prediction 的错误积累。

    可控性与抽象思维：其可控性较差，但因对[[潜空间]]进行特征操作，可能更易理解高层次的抽象思维，在数学和语义思维方面有发展前景。

  **应用探讨**

    思维链应用：王鑫宇认为 diffusion 的加噪逐步优化方式与思维链可并行，但目前未见相关文章，并行可能产生新的思维方式。

    物理过程抽象：Jack Jin 提出如何抽象表征该过程以及适配物理公式的数据格式问题，郭乃绪认为 COT 本质是获得更好答案，在物理过程中可将其视为寻找最小能量态的过程，无需改变模型结构和数据格式，只需考虑下一步迭代。

### RL Wei Xiong

1. **数学推理**
    
    1. 证明工具：除Lean等工具外，当前大模型生成的数学推导对实际研究贡献有限
        
    2. 框架创新：需构建数学知识正则化（K-regularization）体系，与过程奖励（process reward）深度融合
        
    3. 数据瓶颈：人工标注中间步骤成本高昂（如OpenAI方案），DeepSeek的"未来预测"方案存在理论缺口
        
2. 谷歌内部把process做work3-4个月后 R1出来了 感觉不需要process，outcome 就够了。

OAI还是用了process reward ，但是process监督会有人工参与不是纯ai，很需要注意。

3. **关于process reward技术路线 OpenAI与Deepseek的不同：**
    
    1. OpenAI成功要素：人工标注历史轨迹+逐步验证，与DeepSeek纯AI预测方案形成对比
        
    2. 关键差异点：
        
        - 标注方向：人工标注关注历史轨迹验证 vs AI预测基于未来可能性
            
        - 数据质量：人工标注确保中间状态可靠性 vs 自动生成存在误差累积
            
    3. Gemini也是PRM（Process Reward Model），但是没有那么重要

以上是不是主要国内**process reward没跑动的原因，**现在是未解之谜

目前LLM rl推理还解决不了4x4的游戏，比传统deep rl效果差很多

上百轮的交互 outcome reward回传信号会非常差

技术局限性突破点

- 多步交互难题：4x4迷宫任务表现远逊传统DRL，暴露LLM在空间RL上的问题
    
- 长程信号衰减：超百步任务的奖励信号回传效率呈指数级下降
    
- 价值函数重建：目前问题在验证value网络有效性，探索RL框架的规模化应用，make PRM great again

1. test time scaling做一次可能成功率很低 做1000次 可能就解决了90%的问题，我们在改变概率 我们把Path n 蒸馏到path 1 ， Path n不能improve by RL

如何解决这个问题：

1 我的目标是improve path 64 而不是path 1，可以改整个RL的target，框架

2不应该只考虑RL，因为RL initial point是锁死的，initial point是怎么锁死的

LongCoT把LLM从短的space 搜索到更大的 space的搜索，这是一个paradiam的改变

Latent space的改变，人的理解限制了AI自己的潜力，不用强求翻译人的step。但是这么做会容易over fit Yuandong是这里最早的作者 正在scale up，lesson是不应该最后做，1T的math去做CPT，往前走就会不会被锁死

R1就是这么做的 只不过故意不写

4. 大模型RL目前从传统deep rl来看可能只是退火，数据量只有几千太少，未来还是需要能inject new knowledge

Gemini1

Gemini2做出来了 当时MLSys刷榜了 拿了第一

Gemini2.5 内部觉得已经赢了 组里觉得自己赢了

Gemini做一个东西 OAI会狙击谷歌，但是今天已经出来2个月了 OAI也没有新东西

Google DeepMind 3000人

OAI 4000人

Alpha xxx 有些特殊模型已经走得很好了 ，所以可以把这样的Alpha算法拿来goback然后蒸馏他们的大模型

之前o1能拿到 log 概率 next token distribution

O3不行了

**OpenAI Gemini没有蒸馏 他们如何进一步提升**

CPT应该学什么？

math coding agent?

大家现在在做math coding

千问在做tool calling

对这 task 对着 bench

reject sampling合成数据 学一个方向的senario

那o系列 或者 gemini 的推理模型的rl 能做到更长的步数 或者 更好的泛化吗？

RL目前不可能 需要history data 做CPT

2-3年最大的非共识 Domain的speacilzation

模型很快能handle几十上百部的task然后和环境交互

learning paradigm 一定会换

急着毕业 回google 做核心contributor

作为 neurips 的数学负责人 一定要请deepseek的人

会根据leader的靠谱程度去评价一个团队

比如thinking machine

### 《Wei Xiong如何学习》

科大比较solid

1 phd第一年和张潼 他在写一本书 Wei跟着他从很高的视角 proof read了整个20年的历史 两耳不闻窗边事

2 GPT出来，转了RLHF 做工作 比较active的会去跟research ，每天都会搜新的paper 周一到周四 搜RLHF Reasoning

3 RAHF的论文出来， Google（Tianqi Liu 是比较帅 不喜欢宣传） reach out 到他 ，如果当时就是现在这样的的Star。去年给了40个Talk 和很多人聊了，在google做了一个月的工作，后面就开始聊。 需要hands on在做 才能交换价值。

有个后训练微信讨论群

## 工作具体开展

- [[潜空间]]杭州
    
- 线索 Mapping梳理
    
- 行研体系 前沿研究
    
- 内部信息同频
    
- 学习小组